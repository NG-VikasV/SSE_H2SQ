{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b0db2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from typing import List\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba15922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_configs(num_samples, L, spin_values=(-1, 1)):\n",
    "    \"\"\"\n",
    "    Generate random Ising-like configurations.\n",
    "\n",
    "    Args:\n",
    "        num_samples: number of configurations\n",
    "        L: lattice size (L x L)\n",
    "        spin_values: possible spin values (default {-1, +1})\n",
    "    \n",
    "    Returns:\n",
    "        X_in: np.ndarray of shape (num_samples, L, L)\n",
    "    \"\"\"\n",
    "    spins = np.random.choice(spin_values, size=(num_samples, L, L))\n",
    "    return spins\n",
    "\n",
    "def generate_black_plaquettes(L, return_coords=False, periodic=True):\n",
    "    \"\"\"\n",
    "    Generate black plaquette indices for an L x L checkerboard lattice.\n",
    "\n",
    "    A plaquette is defined by 4 sites:\n",
    "      top-left  : (i, j)\n",
    "      top-right : (i, j+1)\n",
    "      bot-right : (i+1, j+1)\n",
    "      bot-left  : (i+1, j)\n",
    "\n",
    "    A plaquette is considered 'black' if (i + j) % 2 == 0 for the top-left corner.\n",
    "\n",
    "    Args:\n",
    "        L (int): lattice linear size (L x L).\n",
    "        return_coords (bool): if True, return list of plaquettes as lists of (i,j) coords.\n",
    "                              if False (default), return list of plaquettes as lists of flattened indices.\n",
    "        periodic (bool): whether to use periodic boundary conditions (wrap edges). Default True.\n",
    "\n",
    "    Returns:\n",
    "        plaquettes (list): list of plaquettes. Each plaquette is length-4 list.\n",
    "                           - If return_coords=True: elements are (i,j) tuples.\n",
    "                           - If return_coords=False: elements are linear indices idx = i*L + j.\n",
    "    \"\"\"\n",
    "    plaquettes = []\n",
    "    for i in range(L):\n",
    "        for j in range(L):\n",
    "            if (i + j) % 2 != 0:\n",
    "                continue\n",
    "            i0, j0 = i, j\n",
    "            i1, j1 = i, (j + 1) % L if periodic else j + 1\n",
    "            i2, j2 = (i + 1) % L if periodic else i + 1, (j + 1) % L if periodic else j + 1\n",
    "            i3, j3 = (i + 1) % L if periodic else i + 1, j\n",
    "\n",
    "            # if not periodic and index out of bounds, skip\n",
    "            if not periodic:\n",
    "                if i1 >= L or j1 >= L or i2 >= L or j2 >= L or i3 >= L or j3 >= L:\n",
    "                    continue\n",
    "\n",
    "            if return_coords:\n",
    "                plaquettes.append([(i0, j0), (i1, j1), (i2, j2), (i3, j3)])\n",
    "            else:\n",
    "                idx0 = i0 * L + j0\n",
    "                idx1 = i1 * L + j1\n",
    "                idx2 = i2 * L + j2\n",
    "                idx3 = i3 * L + j3\n",
    "                plaquettes.append([idx0, idx1, idx2, idx3])\n",
    "    return plaquettes\n",
    "\n",
    "\n",
    "def build_site_to_plaquettes(plaquettes: List[List[int]], N: int) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Build adjacency: site -> list of plaquette indices that contain the site.\n",
    "    Use once after generating plaquettes.\n",
    "    \"\"\"\n",
    "    site2pl = [[] for _ in range(N)]\n",
    "    for p_idx, plaq in enumerate(plaquettes):\n",
    "        for site in plaq:\n",
    "            site2pl[site].append(p_idx)\n",
    "    return site2pl\n",
    "\n",
    "\n",
    "def plaquette_energy_with_diagonals(spins, plaquettes, J0=1.0, J1=1.0):\n",
    "    \"\"\"\n",
    "    Compute full energy from plaquettes with couplings J0, J1.\n",
    "    \n",
    "    spins      : 1D numpy array (length L*L)\n",
    "    plaquettes : list of plaquettes (indices)\n",
    "    \"\"\"\n",
    "    E = 0.0\n",
    "    for plaq in plaquettes:\n",
    "        tl, tr, br, bl = plaq\n",
    "        E +=   -J0 * spins[tl]*spins[tr]*spins[br]*spins[bl] + \\\n",
    "                J1 * (spins[tl]*spins[br] + spins[tr]*spins[bl])\n",
    "    return E\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collect_data(L, T, J0, J1, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate random spin configurations and compute their energies and probabilities.\n",
    "    \n",
    "    Args:\n",
    "        L (int): Lattice size (LxL).\n",
    "        T (float): Temperature.\n",
    "        n_samples (int): Number of random configs.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"configs\": list of np.ndarray (each config shape = (L*L,)),\n",
    "            \"energies\": np.ndarray,\n",
    "            \"probs\": np.ndarray,\n",
    "            \"T\": float\n",
    "        }\n",
    "    \"\"\"\n",
    "    # generate plaquettes once\n",
    "    plaquettes = generate_black_plaquettes(L, return_coords=False)\n",
    "\n",
    "    configs = []\n",
    "    energies = np.zeros(n_samples)\n",
    "    weights = np.zeros(n_samples)\n",
    "    T_list = np.zeros(n_samples)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        spins = np.random.choice([-1, 1], size=L*L)\n",
    "        E = plaquette_energy_with_diagonals(spins, plaquettes, J0, J1)\n",
    "        w = np.exp(-E / T)\n",
    "\n",
    "        configs.append(spins)\n",
    "        energies[i] = E\n",
    "        weights[i] = w\n",
    "        T_list[i] = T\n",
    "\n",
    "    # Normalize probabilities\n",
    "    probs = weights \n",
    "\n",
    "    return {\n",
    "        \"configs\": configs,\n",
    "        \"energies\": energies,\n",
    "        \"probs\": probs,\n",
    "        \"T\": T_list\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3d36526e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plaquettes are:  [[0, 1, 5, 4], [2, 3, 7, 6], [5, 6, 10, 9], [7, 4, 8, 11], [8, 9, 13, 12], [10, 11, 15, 14], [13, 14, 2, 1], [15, 12, 0, 3]]\n",
      "Config:\n",
      " [-1  1  1 -1 -1  1  1  1  1  1  1 -1 -1 -1 -1 -1]\n",
      "Energy: -4.0\n"
     ]
    }
   ],
   "source": [
    "L = 4\n",
    "J0 = 1.0\n",
    "J1 = 0.5\n",
    "num_samples = 2\n",
    "\n",
    "# random configs\n",
    "configs = generate_random_configs(num_samples, L)\n",
    "\n",
    "# plaquettes\n",
    "plaquettes = generate_black_plaquettes(L, return_coords=False, periodic=True)\n",
    "\n",
    "print(\"Plaquettes are: \", plaquettes)\n",
    "\n",
    "# energy of first config\n",
    "conf = configs[0].flatten()\n",
    "E = plaquette_energy_with_diagonals(conf, plaquettes, J0, J1)\n",
    "\n",
    "print(\"Config:\\n\", configs[0].flatten())\n",
    "print(\"Energy:\", E)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0feef719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected data keys: dict_keys(['configs', 'energies', 'probs', 'T'])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    L = 4\n",
    "    T = 0.1\n",
    "    data = collect_data(L, T, J0, J1, n_samples=1000)\n",
    "\n",
    "    print(\"Collected data keys:\", data.keys())\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values(by=\"probs\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "52f4b2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>configs</th>\n",
       "      <th>energies</th>\n",
       "      <th>probs</th>\n",
       "      <th>T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, ...</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2.688117e+43</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, -1,...</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2.688117e+43</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1,...</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2.688117e+43</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, ...</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2.688117e+43</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, -1, ...</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2.688117e+43</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1...</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2.688117e+43</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[1, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1...</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.540622e+34</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[-1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1,...</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.540622e+34</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[-1, -1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1...</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.540622e+34</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1,...</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.540622e+34</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, ...</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.540622e+34</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, -1...</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.540622e+34</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1,...</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.540622e+34</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[-1, -1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 1,...</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.540622e+34</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[1, -1, 1, -1, 1, -1, -1, 1, -1, -1, -1, -1, 1...</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.540622e+34</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[-1, -1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1...</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.540622e+34</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1...</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.540622e+34</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[-1, -1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1...</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.540622e+34</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1...</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.540622e+34</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[-1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, -1, 1,...</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.540622e+34</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              configs  energies         probs  \\\n",
       "0   [-1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, ...     -10.0  2.688117e+43   \n",
       "1   [-1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, -1,...     -10.0  2.688117e+43   \n",
       "2   [-1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1,...     -10.0  2.688117e+43   \n",
       "3   [-1, 1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, ...     -10.0  2.688117e+43   \n",
       "4   [1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, -1, ...     -10.0  2.688117e+43   \n",
       "5   [-1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1...     -10.0  2.688117e+43   \n",
       "6   [1, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1...      -8.0  5.540622e+34   \n",
       "7   [-1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1,...      -8.0  5.540622e+34   \n",
       "8   [-1, -1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1...      -8.0  5.540622e+34   \n",
       "9   [1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1,...      -8.0  5.540622e+34   \n",
       "10  [1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, ...      -8.0  5.540622e+34   \n",
       "11  [1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, -1...      -8.0  5.540622e+34   \n",
       "12  [1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1,...      -8.0  5.540622e+34   \n",
       "13  [-1, -1, 1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 1,...      -8.0  5.540622e+34   \n",
       "14  [1, -1, 1, -1, 1, -1, -1, 1, -1, -1, -1, -1, 1...      -8.0  5.540622e+34   \n",
       "15  [-1, -1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1...      -8.0  5.540622e+34   \n",
       "16  [1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1...      -8.0  5.540622e+34   \n",
       "17  [-1, -1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1...      -8.0  5.540622e+34   \n",
       "18  [1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1...      -8.0  5.540622e+34   \n",
       "19  [-1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, -1, 1,...      -8.0  5.540622e+34   \n",
       "\n",
       "      T  \n",
       "0   0.1  \n",
       "1   0.1  \n",
       "2   0.1  \n",
       "3   0.1  \n",
       "4   0.1  \n",
       "5   0.1  \n",
       "6   0.1  \n",
       "7   0.1  \n",
       "8   0.1  \n",
       "9   0.1  \n",
       "10  0.1  \n",
       "11  0.1  \n",
       "12  0.1  \n",
       "13  0.1  \n",
       "14  0.1  \n",
       "15  0.1  \n",
       "16  0.1  \n",
       "17  0.1  \n",
       "18  0.1  \n",
       "19  0.1  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ac1620e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1, -1,  1, -1,  1, -1,  1, -1,  1,  1,  1, -1,  1, -1, -1])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['configs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e55da35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAHiCAYAAADf3nSgAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJhNJREFUeJzt3UFPFNn+xvGnuqpp2rjBheYSAmZ24jTIuJjoxJegLjSBJW9hVkhc9YKwurlvAXZANJlR2fy3E72ZCFEw6G6SIiQXbwKzuRH6VrfnvygQL6PSNNX166r+fpKTSWQ8KU4/1tNd3X3Kc845AQCA1BWsDwAAgG5FCQMAYIQSBgDACCUMAIARShgAACOUMAAARihhAACMUMIAABihhAEAMEIJAwBghBIGAMAIJQwAgBFKGAAAI5QwAABGKGEAAIxQwgAAGKGEAQAwQgkDAGCEEgYAwAglDACAEUoYAAAjlDAAAEYoYQAAjFDCAAAYoYQBADBCCQMAYIQSBgDACCUMAIARShgAACOUMAAARihhAACMUMIAABihhAEAMEIJAwBghBIGAMAIJQwAgBFKGAAAI5QwAABGKGEAAIxQwgAAGKGEAQAwQgkDAGCEEgYAwEhgfQDtUq/Xtbm5qb29PZXLZQ0ODioIcvvrdgTWPF2sd7pY7/R1w5rn6rfZ3d3V3NycHi8u6tXamvZqtU8/K5dKGhsd1b3xcU1OTurChQuGR5ofh2u+uPhYa2uvVKvtffpZqVTW6OiYxsfvseYJIePpIt/p67qMuxz473//66rVquvt6XFFz3P3JPd3yf2f5F4c/PfvkrsnuaLnud6eHletVt1///tf60PPrMM17+npdZ5XdNI9J/3dSf/npBcH//27k+45zyu6np5e1vwMyHi6yHf6ujXjmS/hMAzdWKXifM9zDyT3XnLuG2NbclOS8z3PjVUqLgxD618hc8IwdJXKmPM830kPnPT+W0vupG0nTTnP812lMsaanxIZTxf5Tl83ZzzTJRyGoRvs73eXg8CtnPCgHR8vJXc5CNxgf3+mH8C0hWHo+vsHXRBcdtLKaZbcSS9dEFx2/f2DrHmTyHi6yHf6uj3jnnPOWV8Sb0UURfrx+nX9+e6dfqvXNdDCHFuSbgWB+q5c0e+rqyoWi0kfZq5EUaTr13/Uu3d/ql7/TWpx1YPglq5c6dPq6u+s+TeQ8XSR7/SRcSmzr4Sr1arzPe/Uz5yOj5WDSxrVatX6V+p41Wr14BLdaV8hHB8rzvN81vwEZDxd5Dt9ZNxl83L0zs6O6+3pcQ/O+MAdjinJ9fb0uJ2dHetfrWPt7Oy4np5eF79HlsSyT7menl7W/CvIeLrId/rIeCyTm3XMzc2pEUX6OaH5fpbUiCLNz88nNGP+zM3NKYoaUoKrHkUN1vwryHi6yHf6yHgskyX8eHFRd5zTxYTmuyTptnN6tLCQ0Iz5s7j4WM7dkRJcdedua2HhUULz5QsZTxf5Th8Zj2WuhOv1ul6trelmwvPelPR6fV2NRiPhmbOvXq9rbe2V1IZVX19/zZofQ8bTRb7TR8aPZG7HrM3NTe3VaqokPG9F0of9fYVhqO+++y7h2bNtc3PzYKeg5Fd9f/+DlpeXNTDQyuci82lra4uMp4h8p4+MH8lcCe/txdvGnU943sP5DufHkaM1ac+q3717N+F584GMp4N82yHjGbwcXS6XJUn/SXjew/kO58eRozVp16rjS8h4Osi3HTKewRIeHBxUuVTSm4TnfSPpXG+vhoaGEp45+wYHB1UqlaW2rHrmIpiKgtqz2mT8r8i3DTIey1xCgiDQ2OioXiQ87wtJ10ZG5Pt+wjNnXxAEGh0dkxJf9eeSvITnzAdP8eokiYx/Gfm2QcZjmSthSbo3Pq4nnqd/JzTfe0lPPU/3JyYSmjF/xsfvyfOeSMmuuqTsfIoxTQ1Jia82Gf8q8p0+Mn7AereQVrRrp5Xd3V3rX61jtWNHIangJDG+MgoSGU8J+SbjVjJZws4lt+foS2V3z9G0Jbe37ktOUKc4SZHxdJBvMm6Buyhl+e4bKUvqLjPSDUn/EpfqTuZL+pukf6r11SbjzSHfNro945l8T1iSisWifnn2TB8vXtStINDqKf/+iuIHzl26pF+XlzP3wFkoFot69uwXXbz4UUFwS2pp1W9I2hYnqOY0FJ/Ob6i11SbjzSPfNro+49Yvxc8qDEM3Vqk43/PclOS2T7hssa34vQPf89xYpZLZG0FbCsPQVSpjB5fuppy0fcLVom139B6Zb375K4vDP7hsR8bbj3yT8TRl9nL056Io0uzsrGZnZtSIIt12TjcVb2F2XvEXuN8o/vj6U8+TXyxq+uFDTU9PZ/OZUwc4XPOZmVlFUUPO3Za+uOrPFX9K9OPBwFkUDsZtST+JjLcL+bbTbRnPRQkf2t3d1fz8vB4tLOj1+ro+7O9/+tm53l5dGxnR/YkJTU5Oqq+vz/BI8+NwzRcWHml9/bX29z989tOC4m8Dcmkuab7ilw+fn/bJePLIt51uyXiuSvhzjUZDYRhqb29P5XJZQ0NDmfoCdxY1Gg0tLy+zV26KhoeHtbS0RMZTQL5t5D3jmbuBQ7N838/MXTTywvd97haTslKppKtXr1ofRlcg3zbynvHMfjoaAICso4QBADBCCQMAYIQSBgDACCUMAIARShgAACOUMAAARihhAACMUMIAABihhAEAMEIJAwBghBIGAMAIJQwAgBFKGAAAI5QwAABGKGEAAIxQwgAAGKGEAQAwQgkDAGCEEgYAwAglDACAEUoYAAAjlDAAAEYoYQAAjFDCAAAYoYQBADBCCQMAYIQSBgDACCUMAIARShgAACOUMAAARihhAACMUMIAABihhAEAMEIJAwBghBIGAMAIJQwAgBFKGAAAI5QwAABGKGEAAIxQwgAAGKGEAQAwQgkDAGCEEgYAwAglDACAEUoYAAAjlDAAAEYoYQAAjFDCAAAYoYQBADBCCQMAYIQSBgDACCUMAIARShgAACOUMAAARihhAACMUMIAABihhAEAMEIJAwBghBIGAMAIJQwAgBFKGAAAI5QwAABGKGEAAIxQwgAAGKGEAQAwQgkDAGAksD6AdqnX69rc3NTe3p7K5bIGBwcVBLn9dTtCvV7X1taW9WF0lVqtpo2NDTKeAvJtI+8Z95xzzvogkrK7u6u5uTk9XlzUq7U17dVqn35WLpU0Njqqe+Pjmpyc1IULFwyPND8O13xx8bHW1l6pVtv77KcFSZ6khtHR5ZcvyUn6+NmfkfHkkW873ZLxXJRwFEWanZ3V7MyMGlGkO87ppqSKpPOS/iPpjaQXkp54nvxiUdMPH2p6elrFYtHy0DPrcM1nZmYVRQ05d0f64qo/l/RE8T+lj1+dD80pHIw7kn4SGW8X8m2n6zLuMi4MQzdWqTjf89wDyb2XnPvG2JbclOR8z3NjlYoLw9D6V8icMAxdpTLmPM930gMnvf/Wkjtp20lTTio4yXeKn+AyTjF8yRUkMp4C8k3G05TpEg7D0A3297vLQeBWTnjQjo+XkrscBG6wvz/TD2DawjB0/f2DLgguO2nlNEvupJdOGuBEdcoRSG5AIuMpIN9kPG2ZvRwdRZF+vH5df757p9/qdQ20MMeWpFtBoL4rV/T76mq2L2mkIIoiXb/+o969+1P1+m9Sy6t+Q9K/xHtpJ/Ml/U3SP9X6apPx5pBvG12fcetnAa2qVqvO97xTP3M6PlYUX9KoVqvWv1LHq1arB5foTvsK4fhYcfGlO/tn4J0+Ci28OiDjrSHfZNxCJkt4Z2fH9fb0uAdnfOAOx5Tkent63M7OjvWv1rF2dnZcT0+vi98jS2LZD99Dsz8JdOo4fH+MjLcf+SbjVjK5Wcfc3JwaUaSfE5rvZ0mNKNL8/HxCM+bP3NycoqghJbrqmYxfagpKdrXJ+NeRbxtkPKMpeby4qDvO6WJC812SdNs5PVpYSGjG/FlcfKz4axqJrrrid4RwnK/4KxpkPB3kO31kPJa5Eq7X63q1tqabCc97U9Lr9XU1GnyY4rh6va61tVdS4qv+k+KrUjjOKV6dJJHxLyPfNsh4LHP7f21ubmqvVlMl4Xkrkj7s72t5eVkDA618Ri+/tra2DnYKaseqf9Tw8LBKpVLCc2dXrVbT27dvyXhKyHf6uiXjP/zww4n/T+ZKeG8v3jbufMLzHs539+7dhGfOk/as+tLSkq5evZrw3Nm1sbGh77//noynjnynpVsy7tzJV0Iydzm6XC5LircwS1LS8+VTe1b98DFFjIxbId9pIeNHMlfCg4ODKpdKepPwvG+UwcVIVUFqw6r39p7T0NBQwvNmGxm3QL7TRMaPZO14FQSBxkZH9SLheZ8rvh8KvsZTvEpJeqGRkWvyfT5B+jkyboF8p4mMH8lcCUvSvfFxPfE8/Tuh+d5Leio2mfu2huK7xSS36p73VBMT9xOaL1/IeNrId9rIeCyTJTw5OSm/WNQ/EprvH+ImZM35KCW46sWir8nJyYTmyxcyboF8p4mMH7DesqtVSe0d/fJg6zQxmhwFd/a9dV86z/Mzuc9rmsg4+c67vGe8GV1/FyXud3JaZ7/nSRDc0pUrfVpd/T17dzxJERm3QL7TlPeMN1OvmbwcLUnFYlG/PHumjxcv6lYQaPWUf39F8QO3rc574DpbQ3Hcb0gtrHoQ3NKlS07Ly79ygjoBGbdAvtNExtXk6+UOFoahG6tUnO95bkpy2ydctthWfLeNguT8Drhckd3hu/jS3ZSTtr+15Ac/n3Ke57tKZSyzN9+2QsbJd97lNePNyOzl6M9FUaTZ2VnNzsyoEUW67ZxuKt7C7LziL3C/Ufzx9aeK37zP5Bv4HalwMG4r3gn2+Kq/kOc9VbHo6+HDaU1PT/MKoQVk3Ar5TkseM95MveaihA/t7u5qfn5ejxYW9Hp9XR/29z/9rKD4+2OZvWTR8XzFT/6O/ln09p7TyMg1TUzc1+TkpPr6+syOLi/IuBXynZY8ZbzrSvhzjUZDy8vLHbOHaLcYHh7W0tKSyuWyhoaG2Kigjch4+sh3urKe8WbqNXM3cGiW7/sdcReNblMqldisPiVkPH3kO13dkPHMfjoaAICso4QBADBCCQMAYIQSBgDACCUMAIARShgAACOUMAAARihhAACMUMIAABihhAEAMEIJAwBghBIGAMAIJQwAgBFKGAAAI5QwAABGKGEAAIxQwgAAGKGEAQAwQgkDAGCEEgYAwAglDACAEUoYAAAjlDAAAEYoYQAAjFDCAAAYoYQBADBCCQMAYIQSBgDACCUMAIARShgAACOUMAAARihhAACMUMIAABihhAEAMEIJAwBghBIGAMAIJQwAgBFKGAAAI5QwAABGKGEAAIxQwgAAGKGEAQAwQgkDAGCEEgYAwAglDACAEUoYAAAjlDAAAEYoYQAAjFDCAAAYoYQBADBCCQMAYIQSBgDACCUMAIARShgAACOUMAAARihhAACMUMIAABihhAEAMEIJAwBghBIGAMAIJQwAgBFKGAAAI5QwAABGKGEAAIxQwgAAGKGEAQAwktsSrtfr2trasj6MrlOr1bSxsaE//vhD9Xrd+nByjYynj3ynqxsy7jnnnPVBJGV3d1dzc3N6vLioV2tr2qvVPv2sIMmT1DA7urzzJTlJHz/9SalU1ujomMbH72lyclIXLlwwO7q8OMz44uJjra29Uq2299lPSXn7kO+05Ok83ky95qKEoyjS7OysZmdm1Igi3XFONyVVJJ2X9B9JbyQ9l/RE8T+jj1+fDqdSOBh3JP2kv676C3neExWLvh4+nNb09LSKxaLd4WbUYcZnZmYVRQ05d0ci5Skg32nJ43m8qXp1GReGoRurVJzvee6B5N5Lzn1jbEtuSnIFyfnxU1tGS8N3UsFJD5z0/ltL7qRtJ005z/NdpTLmwjC0jk2mhGHoKpUx53n+qdY7fnz8DshKFgf5TlNez+PNyHQJh2HoBvv73eUgcCsnPGjHx0vJDXT4A9i5I3DSgJNWTrPkTnrpguCy6+8f5ETVpDAMXX//oAuCyy2td/w4UcTku3Pl+TzejMxejo6iSD9ev64/373Tb/W6BlqYY0vSDUn/UnbeY7DnS/qbpH9KLa56ENzSlSt9Wl39nUt33xBFka5f/1Hv3v2pev03tbrepPw0yHea8n4eb6pe2/kMp52q1arzPe/Uz5yOjxXFlzTEaHIU3OlfIRwfK87zfFetVq1j1NGq1erBJeizr3f8uFlnJwuDfKcp7+fxZmSyhHd2dlxvT497cMYH7nAcvrdg/YB1/jh8jyyJZZ9yPT29bmdnxzpOHWlnZ8f19PQmut4UMfnuJN1wHm9GJr8nPDc3p0YU6eeE5vtZOf7CdKIKUoKrHkUNzc/PJzRfvszNzSmKGkpyvUn5Sch3mjiPx7J4zHq8uKg7zuliQvNdknRb8btB+Bpf8dc0klt1525rYeFRQvPly+LiY8VfQyLl6SDfaeM8HstcCdfrdb1aW9PNhOf9SfH1A3yNU7xKSbqp9fXXajQ67eMUtur1utbWXkmkPEXkO02cx48E1gdwWpubm9qr1VRJeN6K4i9+Dw8Pq1QqJTx7ttVqNb19+1Zqw6rv73/Q8vKyBgZa+VxkPm1tbR3shNWelJPx/0W+07e1tcV5/EDmSnhvL96m73zC8x7Ot7S0pKtXryY8e7ZtbGzo+++/V7tW/e7duwnPmxftWW8y/r/Itx3O4xm8HF0ulyXFW5gl6XC+w/lx5GhN2rXq+LL2rDcZ/1/k2w7n8QyW8ODgoMqlkt4kPO8bSed6ezU0NJTwzNk3ODioUqkstWXVMxfBlBTUjvXu7T1Hxo8h3zbak/Dsncczl5AgCDQ2OqoXCc/7QtK1kRH5ftY+W9d+QRBodHRMSnzVnyu+Jwr+ylO8Pkl6oZGRa2T8GPJtoz0Jz955PHMlLEn3xsf1xPP074Tmey/pqefp/sREQjPmz/j4PXneEynZVVfnbTTXKRqK7xWT3Hp73lNNTNxPaL58Id/pSz7hGT2Pt3lTlLZox04rvT09bnd31/pX61js4JSPHZzI+JeRb5tRkLr+PJ7JEnYuuT1HX0rO9zz2eW1CcnsZv+QE1fRIYi/jl+xl3ATybTMKUlefx7v+Lkq3gkB9V67o99VV7nhyAu7qY4G7+qSFfNs4e8KzfR7P5HvCklQsFvXLs2f6ePGibgWBVk/591cUP3Du0iX9urycuQfOQrFY1LNnv+jixY8KgltSS6t+Q9K2OEE1q6H4hH5Drax3ENzSpUtOy8u/kvETkG8bZ0t4Ds7j1i/FzyoMQzdWqTjf89yU5LZPuGyxrfi9A9/z3Filws23WxCGoatUxg4u3U05afuEq0Xb7ug9Mm4w39rwD9av+fX2PN9VKmNk/JTIt83wFV+a7rbzeGYvR38uiiLNzs5qdmZGjSjSbed0U/EWZucVf4H7jeKPrz/1PPnFoqYfPtT09HQ2nzl1gMM1n5mZVRQ15Nxt6Yur/lzxp0Q/HgycTeFg3Fa8U+5fU+55T1Us+nr4cJqMt4h82zk54fk6j+eihA/t7u5qfn5ejxYW9Hp9XR/29z/97Fxvr66NjOj+xIQmJyfV19dneKT5cbjmCwuPtL7+Wvv7Hz77aUHxtwG5NJc8X/ELiKMTf2/vOY2MXNPExH0ynhDybeevCc/neTxXJfy5RqOhMAy1t7encrmsoaGhTH2BO4sajYaWl5fZKzdFw8PDWlpaIuMpIN828p7xzN3AoVm+7+u7776zPoyu4vs+d4tJWalUysxG9VlHvm3kPeOZ/XQ0AABZRwkDAGCEEgYAwAglDACAEUoYAAAjlDAAAEYoYQAAjFDCAAAYoYQBADBCCQMAYIQSBgDACCUMAIARShgAACOUMAAARihhAACMUMIAABihhAEAMEIJAwBghBIGAMAIJQwAgBFKGAAAI5QwAABGKGEAAIxQwgAAGKGEAQAwQgkDAGCEEgYAwAglDACAEUoYAAAjlDAAAEYoYQAAjFDCAAAYoYQBADBCCQMAYIQSBgDACCUMAIARShgAACOUMAAARihhAACMUMIAABihhAEAMEIJAwBghBIGAMAIJQwAgBFKGAAAI5QwAABGKGEAAIxQwgAAGKGEAQAwQgkDAGCEEgYAwAglDACAEUoYAAAjlDAAAEYoYQAAjFDCAAAYoYQBADBCCQMAYIQSBgDACCUMAIARShgAACOUMAAARihhAACMUMIAABihhAEAMEIJAwBgJLA+gHap1+va3NzU3t6eyuWyBgcHFQS5/XU7Qr1e19bWlvVhdJVaraaNjQ0yngLybSPvGfecc876IJKyu7urubk5PV5c1Ku1Ne3Vap9+Vi6VNDY6qnvj45qcnNSFCxcMjzQ/Dtd8cfGx1tZeqVbb++ynBUmepIbR0eWXL8lJ+vjZn5Hx5H3rnEK626tbMp6LEo6iSLOzs5qdmVEjinTHOd2UVJF0XtJ/JL2R9ELSE8+TXyxq+uFDTU9Pq1gsWh56Zh2u+czMrKKoIefuSF9c9eeSnij+p/Txq/OhOYWDcUfSTyLj7dLsOYV0J6/rMu4yLgxDN1apON/z3APJvZec+8bYltyU5HzPc2OVigvD0PpXyJwwDF2lMuY8z3fSAye9/9aSO2nbSVNOKjjJd4qf4DJOMXzJFSQynoJWzymFg8fJOitZHd2a8UyXcBiGbrC/310OArdywoN2fLyU3OUgcIP9/Zl+ANMWhqHr7x90QXDZSSunWXInvXTSgKOITzcCyQ1IZDwFZz2nDIgiJuOnk9nL0VEU6cfr1/Xnu3f6rV7XQAtzbEm6FQTqu3JFv6+uZvuSRgqiKNL16z/q3bs/Va//JrW86jck/Uu8m3YyX9LfJP1Tra82GW9OUucU0n06XZ9x62cBrapWq873vFM/czo+VhRf0qhWq9a/UserVqsHl6BP+wr4+Fhx8aVp+2fgnT4KLbw6IOOtSfKcUuiA7GRldHvGM1nCOzs7rrenxz044wN3OKYk19vT43Z2dqx/tY61s7Pjenp6XfwecBLLfvgesf1JoFPH4ftjZLz92nFOoYjJeDMyuVnH3NycGlGknxOa72dJjSjS/Px8QjPmz9zcnKKoISW66pmMX2oKSna1yfjXteOcQrpPRsYzmpPHi4u645wuJjTfJUm3ndOjhYWEZsyfxcXHir+GlOiqK35HCMf5ir+iQcbT0ZZzikj3t5DxWOZKuF6v69Xamm4mPO9NSa/X19Vo8HGK4+r1utbWXkmJr/pPiq9K4TineHWSRMa/rF3nFNL9bWQ8lrn9vzY3N7VXq6mS8LwVSR/297W8vKyBgVY+o5dfW1tbBzthtWPVP2p4eFilUinhubOrVqvp7du3ZDwlW1tbbTunfJTI9xd0S8Z/+OGHE/+fzJXw3l68LeL5hOc9nO/u3bsJz5wn7Vn1paUlXb16NeG5s2tjY0Pff/89GU9Zu9abfP9Vt2TcuZOvhWTucnS5XJYUb2GWpKTny6f2rPrhY4oYGbfRrvUm339Fxo9kroQHBwdVLpX0JuF53yiDi5GqgtSGVe/tPaehoaGE5802Mp6+9qRbOtfbS76/gIwfydrxKggCjY2O6kXC8z5XfEcUfI2neJWS9EIjI9fk+3yG9HNkPH3tSbd0bWSEfH8BGT+SuRKWpHvj43riefp3QvO9l/RUbDP3bQ3F94tJbtU976kmJu4nNF++kPF0JZ9u6ann6f7EREIz5g8Zj2WyhCcnJ+UXi/pHQvP9Q9yGrDkfpQRXvVj0NTk5mdB8+ULG05dsuiW/WCTf30DGD1hv2dWqpPZ5fcn2cqccBXf2vaNfOs/zM7nPa5rIePojiX2MXyq7+xinLe8Zb0bX30WJO56c1tnveRIEt3TlSp9WV3/P3h1PUkTG09f1d/RJWd4z3ky9ZvJytCQVi0X98uyZPl68qFtBoNVT/v0VxQ/ctjrvgetsDcVxvyG1sOpBcEuXLjktL//KCeoEZDx9Z0t3XMDu0iX9urxMvptAxtXk6+UOFoahG6tUnO95bkpy2ydcttjW0R1OuPn2WYbv4kvTU07a/taSH/x8ynme7yqVsczefNsKGU9/+Afrd5r19j3PjVUq5LsFec14MzJ7OfpzURRpdnZWszMzakSRbjunm4q3MDuv+AvcbxR/fP2p4jfvM/kGfkcqHIzbineCPb7qL+R5T1Us+nr4cFrT09O8QmgBGbdxcrrjT0H7xaKmHz4k32eQx4w3U6+5KOFDu7u7mp+f16OFBb1eX9eH/f1PPyso/v5YZi9ZdDxf8ZO/o38Wvb3nNDJyTRMT9zU5Oam+vj6zo8sLMm7jr+mON+K4NjKi+xMT5DtBecp415Xw5xqNhpaXlztmD9FuMTw8rKWlJZXLZQ0NDbFRQRuR8fSR73RlPePN1GvmbuDQLN/3O+IuGt2mVCqxWX1KyHj6yHe6uiHjmf10NAAAWUcJAwBghBIGAMAIJQwAgBFKGAAAI5QwAABGKGEAAIxQwgAAGKGEAQAwQgkDAGCEEgYAwAglDACAEUoYAAAjlDAAAEYoYQAAjFDCAAAYoYQBADBCCQMAYIQSBgDACCUMAIARShgAACOUMAAARihhAACMUMIAABihhAEAMEIJAwBghBIGAMAIJQwAgBFKGAAAI5QwAABGKGEAAIxQwgAAGKGEAQAwQgkDAGCEEgYAwAglDACAEUoYAAAjlDAAAEYoYQAAjFDCAAAYoYQBADBCCQMAYIQSBgDACCUMAIARShgAACOUMAAARihhAACMUMIAABihhAEAMEIJAwBghBIGAMAIJQwAgBFKGAAAI5QwAABGKGEAAIxQwgAAGKGEAQAwQgkDAGCEEgYAwAglDACAEUoYAAAjlDAAAEYoYQAAjFDCAAAYoYQBADBCCQMAYIQSBgDASG5LuF6va2try/owuk6tVtPGxob++OMP1et168PJNTKePvKdrm7IuOecc9YHkZTd3V3Nzc3p8eKiXq2taa9W+/SzgiRPUsPs6PLOl+Qkffz0J6VSWaOjYxofv6fJyUlduHDB7OjygoxbId9pyVPGm6nXXJRwFEWanZ3V7MyMGlGkO87ppqSKpPOS/iPpjaTnkp4o/mf08evT4VQKB+OOpJ/011V/Ic97omLR18OH05qenlaxWLQ73Iwi41bId1rymPGm6tVlXBiGbqxScb7nuQeSey85942xLbkpyRUk58dPbRktDd9JBSc9cNL7by25k7adNOU8z3eVypgLw9A6NplCxsl33uU1483IdAmHYegG+/vd5SBwKyc8aMfHS8kNdPgD2LkjcNKAk1ZOs+ROeumC4LLr7x/kRNUkMk6+8y7PGW9GZi9HR1GkH69f15/v3um3el0DLcyxJemGpH8pO+8x2PMl/U3SP6UWVz0IbunKlT6trv7OpbtvIOMWyHea8p7xpuq1nc9w2qlarTrf8079zOn4WFF8SUOMJkfBnf4VwvGx4jzPd9Vq1TpGHY2Mk++8y3vGm5HJEt7Z2XG9PT3uwRkfuMNx+N6C9QPW+ePwPbIkln3K9fT0up2dHes4dSQyTr7zrhsy3oxMfk94bm5OjSjSzwnN97Ny/IXpRBWkBFc9ihqan59PaL58IeMWyHeayHgsi8esx4uLuuOcLiY03yVJtxW/G4Sv8RV/TSO5VXfuthYWHiU0X76Q8bSR77SR8VjmSrher+vV2ppuJjzvT4qvH+BrnOJVStJNra+/VqPRaR+nsEXGLZDvNJHxI4H1AZzW5uam9mo1VRKet6L4i9/Dw8MqlUoJz55ttVpNb9++ldqw6vv7H7S8vKyBgVY+F5lPW1tbZDxF5Dt9ZPxI5kp4b29PUryDSpIO51taWtLVq1cTnj3bNjY29P3336tdq3737t2E580HMp4O8m2HjGfwcnS5XJYUb2GWpMP5DufHkaM1adeq40vIeDrItx0ynsESHhwcVLlU0puE530j6Vxvr4aGhhKeOfsGBwdVKpWltqx65iKYioLas9pk/K/Itw0yHstcQoIg0NjoqF4kPO8LSddGRuT7WftsXfsFQaDR0TEp8VV/rvieKDjOU7w6SSLjX0a+bZDxWOZKWJLujY/riefp3wnN917SU8/T/YmJhGbMn/Hxe/K8J1Kyq67O22iuMzQU3ymGjKeDfKePjB9o86YobdGOnVZ6e3rc7u6u9a/WsXZ2dlxPT2+iOwrFOxTZ72rTqaMgkfGUkG8ybiWTJexccnuOvpSc73ns89qEarXqPM93Z99b9yUnqFOcpMh4Osg3GbfQ9XdRuhUE6rtyRb+vrnLHkxNEUaTr13/Uu3d/ql7/Ta3eZaZz73nSec5+Tx8y3izybaPbM57J94QlqVgs6pdnz/Tx4kXdCgKtnvLvryh+4NylS/p1eTlzD5yFYrGoZ89+0cWLHxUEt6SWVv2GpG1xgmpOQ/Hp/IZaW20y3jzybaPrM279UvyswjB0Y5WK8z3PTUlu+4TLFtuK3zvwPc+NVSrcfLsFYRi6SmXs4NLdlJO2T7hatO2O3iPzzS9/ZXH4B5ftyHj7kW8ynqbMXo7+XBRFmp2d1ezMjBpRpNvO6abiLczOK/4C9xvFH19/6nnyi0VNP3yo6enpbD5z6gCHaz4zM6soasi529IXV/254k+JfjwYOIvCwbiteJ9cMt4e5NtOt2U8FyV8aHd3V/Pz83q0sKDX6+v6sL//6Wfnent1bWRE9ycmNDk5qb6+PsMjzY/DNV9YeKT19dfa3//w2U8Lir8NyKW5pPmKXz58fton48kj33a6JeO5KuHPNRoNhWGovb09lctlDQ0NZeoL3FnUaDS0vLzMXrkpGh4e1tLSEhlPAfm2kfeMZ+4GDs3yfV/fffed9WF0Fd/3uVtMykqlUmY2qs868m0j7xnP7KejAQDIOkoYAAAjlDAAAEYoYQAAjFDCAAAYoYQBADBCCQMAYIQSBgDACCUMAIARShgAACOUMAAARihhAACMUMIAABihhAEAMEIJAwBghBIGAMAIJQwAgBFKGAAAI5QwAABGKGEAAIxQwgAAGKGEAQAwQgkDAGCEEgYAwAglDACAEUoYAAAjlDAAAEYoYQAAjFDCAAAYoYQBADBCCQMAYIQSBgDAiOecc9YHAQBAN+KVMAAARihhAACMUMIAABihhAEAMEIJAwBghBIGAMAIJQwAgBFKGAAAI5QwAABG/h9YwT7GSevBKQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_spin_config_checkerboard_pbc(spins, L):\n",
    "    spins = np.array(spins).reshape(L, L)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "    # draw checkerboard with PBC (extra row + column)\n",
    "    for i in range(L):\n",
    "        for j in range(L):\n",
    "            if (i + j) % 2 == 0:\n",
    "                ax.add_patch(plt.Rectangle((j, L-1-i), 1, 1, color=\"white\"))\n",
    "            else:\n",
    "                ax.add_patch(plt.Rectangle((j, L-1-i), 1, 1, color=\"black\"))\n",
    "\n",
    "    # add extra row and column of plaquettes\n",
    "    for j in range(L):\n",
    "        if (L + j) % 2 == 0:\n",
    "            ax.add_patch(plt.Rectangle((j, -1), 1, 1, color=\"white\"))\n",
    "        else:\n",
    "            ax.add_patch(plt.Rectangle((j, -1), 1, 1, color=\"black\"))\n",
    "\n",
    "    for i in range(L):\n",
    "        if (i + L) % 2 == 0:\n",
    "            ax.add_patch(plt.Rectangle((L, L-1-i), 1, 1, color=\"white\"))\n",
    "        else:\n",
    "            ax.add_patch(plt.Rectangle((L, L-1-i), 1, 1, color=\"black\"))\n",
    "\n",
    "    # corner plaquette\n",
    "    ax.add_patch(plt.Rectangle((L, -1), 1, 1, \n",
    "                               color=\"black\" if (2*L) % 2 else \"white\"))\n",
    "\n",
    "    # plot spins at corners including PBC copies\n",
    "    for i in range(L+1):\n",
    "        for j in range(L+1):\n",
    "            si = spins[i % L, j % L]  # wrap around\n",
    "            color = \"red\" if si == -1 else \"blue\"\n",
    "            ax.scatter(j, L-i, c=color, s=250, edgecolors=\"k\", zorder=3)\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xlim(-0.5, L+0.5)\n",
    "    ax.set_ylim(-0.5, L+0.5)\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example\n",
    "L = 4\n",
    "spins = [1, -1, -1, -1,\n",
    "         1, -1, 1, 1,\n",
    "         -1, -1, 1, -1,\n",
    "         1, 1, -1, 1]  # length L*L = 16\n",
    "\n",
    "plot_spin_config_checkerboard_pbc(df['configs'][0], L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8fd03edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.5230\n",
      "Epoch 2, Loss: 1.5127\n",
      "Epoch 3, Loss: 1.5077\n",
      "Epoch 4, Loss: 1.5038\n",
      "Epoch 5, Loss: 1.4977\n",
      "Epoch 6, Loss: 1.4845\n",
      "Epoch 7, Loss: 1.4589\n",
      "Epoch 8, Loss: 1.4187\n",
      "Epoch 9, Loss: 1.3538\n",
      "Epoch 10, Loss: 1.2742\n",
      "Epoch 11, Loss: 1.1868\n",
      "Epoch 12, Loss: 1.1040\n",
      "Epoch 13, Loss: 1.0276\n",
      "Epoch 14, Loss: 0.9691\n",
      "Epoch 15, Loss: 0.9169\n",
      "Epoch 16, Loss: 0.8795\n",
      "Epoch 17, Loss: 0.8491\n",
      "Epoch 18, Loss: 0.8139\n",
      "Epoch 19, Loss: 0.7790\n",
      "Epoch 20, Loss: 0.7339\n",
      "Epoch 21, Loss: 0.6893\n",
      "Epoch 22, Loss: 0.6333\n",
      "Epoch 23, Loss: 0.5844\n",
      "Epoch 24, Loss: 0.5346\n",
      "Epoch 25, Loss: 0.4882\n",
      "Epoch 26, Loss: 0.4409\n",
      "Epoch 27, Loss: 0.3961\n",
      "Epoch 28, Loss: 0.3584\n",
      "Epoch 29, Loss: 0.3193\n",
      "Epoch 30, Loss: 0.2873\n",
      "Epoch 31, Loss: 0.2576\n",
      "Epoch 32, Loss: 0.2302\n",
      "Epoch 33, Loss: 0.2078\n",
      "Epoch 34, Loss: 0.1869\n",
      "Epoch 35, Loss: 0.1687\n",
      "Epoch 36, Loss: 0.1525\n",
      "Epoch 37, Loss: 0.1377\n",
      "Epoch 38, Loss: 0.1236\n",
      "Epoch 39, Loss: 0.1111\n",
      "Epoch 40, Loss: 0.1014\n",
      "Epoch 41, Loss: 0.0906\n",
      "Epoch 42, Loss: 0.0822\n",
      "Epoch 43, Loss: 0.0746\n",
      "Epoch 44, Loss: 0.0672\n",
      "Epoch 45, Loss: 0.0608\n",
      "Epoch 46, Loss: 0.0556\n",
      "Epoch 47, Loss: 0.0511\n",
      "Epoch 48, Loss: 0.0458\n",
      "Epoch 49, Loss: 0.0420\n",
      "Epoch 50, Loss: 0.0385\n",
      "Epoch 51, Loss: 0.0352\n",
      "Epoch 52, Loss: 0.0324\n",
      "Epoch 53, Loss: 0.0297\n",
      "Epoch 54, Loss: 0.0273\n",
      "Epoch 55, Loss: 0.0250\n",
      "Epoch 56, Loss: 0.0231\n",
      "Epoch 57, Loss: 0.0217\n",
      "Epoch 58, Loss: 0.0198\n",
      "Epoch 59, Loss: 0.0185\n",
      "Epoch 60, Loss: 0.0171\n",
      "Epoch 61, Loss: 0.0158\n",
      "Epoch 62, Loss: 0.0151\n",
      "Epoch 63, Loss: 0.0142\n",
      "Epoch 64, Loss: 0.0133\n",
      "Epoch 65, Loss: 0.0127\n",
      "Epoch 66, Loss: 0.0118\n",
      "Epoch 67, Loss: 0.0110\n",
      "Epoch 68, Loss: 0.0106\n",
      "Epoch 69, Loss: 0.0100\n",
      "Epoch 70, Loss: 0.0095\n",
      "Epoch 71, Loss: 0.0090\n",
      "Epoch 72, Loss: 0.0085\n",
      "Epoch 73, Loss: 0.0083\n",
      "Epoch 74, Loss: 0.0078\n",
      "Epoch 75, Loss: 0.0074\n",
      "Epoch 76, Loss: 0.0073\n",
      "Epoch 77, Loss: 0.0071\n",
      "Epoch 78, Loss: 0.0067\n",
      "Epoch 79, Loss: 0.0064\n",
      "Epoch 80, Loss: 0.0061\n",
      "Epoch 81, Loss: 0.0060\n",
      "Epoch 82, Loss: 0.0057\n",
      "Epoch 83, Loss: 0.0054\n",
      "Epoch 84, Loss: 0.0055\n",
      "Epoch 85, Loss: 0.0055\n",
      "Epoch 86, Loss: 0.0052\n",
      "Epoch 87, Loss: 0.0049\n",
      "Epoch 88, Loss: 0.0047\n",
      "Epoch 89, Loss: 0.0046\n",
      "Epoch 90, Loss: 0.0045\n",
      "Epoch 91, Loss: 0.0044\n",
      "Epoch 92, Loss: 0.0041\n",
      "Epoch 93, Loss: 0.0041\n",
      "Epoch 94, Loss: 0.0040\n",
      "Epoch 95, Loss: 0.0039\n",
      "Epoch 96, Loss: 0.0038\n",
      "Epoch 97, Loss: 0.0038\n",
      "Epoch 98, Loss: 0.0036\n",
      "Epoch 99, Loss: 0.0035\n",
      "Epoch 100, Loss: 0.0035\n",
      "Epoch 101, Loss: 0.0033\n",
      "Epoch 102, Loss: 0.0034\n",
      "Epoch 103, Loss: 0.0033\n",
      "Epoch 104, Loss: 0.0032\n",
      "Epoch 105, Loss: 0.0031\n",
      "Epoch 106, Loss: 0.0032\n",
      "Epoch 107, Loss: 0.0032\n",
      "Epoch 108, Loss: 0.0030\n",
      "Epoch 109, Loss: 0.0028\n",
      "Epoch 110, Loss: 0.0027\n",
      "Epoch 111, Loss: 0.0027\n",
      "Epoch 112, Loss: 0.0029\n",
      "Epoch 113, Loss: 0.0027\n",
      "Epoch 114, Loss: 0.0025\n",
      "Epoch 115, Loss: 0.0024\n",
      "Epoch 116, Loss: 0.0024\n",
      "Epoch 117, Loss: 0.0027\n",
      "Epoch 118, Loss: 0.0028\n",
      "Epoch 119, Loss: 0.0025\n",
      "Epoch 120, Loss: 0.0024\n",
      "Epoch 121, Loss: 0.0026\n",
      "Epoch 122, Loss: 0.0027\n",
      "Epoch 123, Loss: 0.0022\n",
      "Epoch 124, Loss: 0.0022\n",
      "Epoch 125, Loss: 0.0022\n",
      "Epoch 126, Loss: 0.0022\n",
      "Epoch 127, Loss: 0.0019\n",
      "Epoch 128, Loss: 0.0020\n",
      "Epoch 129, Loss: 0.0020\n",
      "Epoch 130, Loss: 0.0021\n",
      "Epoch 131, Loss: 0.0019\n",
      "Epoch 132, Loss: 0.0021\n",
      "Epoch 133, Loss: 0.0021\n",
      "Epoch 134, Loss: 0.0020\n",
      "Epoch 135, Loss: 0.0020\n",
      "Epoch 136, Loss: 0.0018\n",
      "Epoch 137, Loss: 0.0018\n",
      "Epoch 138, Loss: 0.0019\n",
      "Epoch 139, Loss: 0.0023\n",
      "Epoch 140, Loss: 0.0020\n",
      "Epoch 141, Loss: 0.0020\n",
      "Epoch 142, Loss: 0.0020\n",
      "Epoch 143, Loss: 0.0020\n",
      "Epoch 144, Loss: 0.0019\n",
      "Epoch 145, Loss: 0.0019\n",
      "Epoch 146, Loss: 0.0019\n",
      "Epoch 147, Loss: 0.0019\n",
      "Epoch 148, Loss: 0.0015\n",
      "Epoch 149, Loss: 0.0015\n",
      "Epoch 150, Loss: 0.0015\n",
      "Epoch 151, Loss: 0.0016\n",
      "Epoch 152, Loss: 0.0017\n",
      "Epoch 153, Loss: 0.0017\n",
      "Epoch 154, Loss: 0.0018\n",
      "Epoch 155, Loss: 0.0015\n",
      "Epoch 156, Loss: 0.0015\n",
      "Epoch 157, Loss: 0.0017\n",
      "Epoch 158, Loss: 0.0015\n",
      "Epoch 159, Loss: 0.0016\n",
      "Epoch 160, Loss: 0.0019\n",
      "Epoch 161, Loss: 0.0014\n",
      "Epoch 162, Loss: 0.0013\n",
      "Epoch 163, Loss: 0.0015\n",
      "Epoch 164, Loss: 0.0014\n",
      "Epoch 165, Loss: 0.0015\n",
      "Epoch 166, Loss: 0.0014\n",
      "Epoch 167, Loss: 0.0017\n",
      "Epoch 168, Loss: 0.0015\n",
      "Epoch 169, Loss: 0.0014\n",
      "Epoch 170, Loss: 0.0013\n",
      "Epoch 171, Loss: 0.0012\n",
      "Epoch 172, Loss: 0.0013\n",
      "Epoch 173, Loss: 0.0014\n",
      "Epoch 174, Loss: 0.0013\n",
      "Epoch 175, Loss: 0.0013\n",
      "Epoch 176, Loss: 0.0017\n",
      "Epoch 177, Loss: 0.0015\n",
      "Epoch 178, Loss: 0.0016\n",
      "Epoch 179, Loss: 0.0015\n",
      "Epoch 180, Loss: 0.0012\n",
      "Epoch 181, Loss: 0.0012\n",
      "Epoch 182, Loss: 0.0014\n",
      "Epoch 183, Loss: 0.0011\n",
      "Epoch 184, Loss: 0.0013\n",
      "Epoch 185, Loss: 0.0016\n",
      "Epoch 186, Loss: 0.0014\n",
      "Epoch 187, Loss: 0.0011\n",
      "Epoch 188, Loss: 0.0012\n",
      "Epoch 189, Loss: 0.0012\n",
      "Epoch 190, Loss: 0.0013\n",
      "Epoch 191, Loss: 0.0010\n",
      "Epoch 192, Loss: 0.0012\n",
      "Epoch 193, Loss: 0.0013\n",
      "Epoch 194, Loss: 0.0012\n",
      "Epoch 195, Loss: 0.0011\n",
      "Epoch 196, Loss: 0.0011\n",
      "Epoch 197, Loss: 0.0014\n",
      "Epoch 198, Loss: 0.0011\n",
      "Epoch 199, Loss: 0.0011\n",
      "Epoch 200, Loss: 0.0012\n",
      "Epoch 201, Loss: 0.0019\n",
      "Epoch 202, Loss: 0.0017\n",
      "Epoch 203, Loss: 0.0014\n",
      "Epoch 204, Loss: 0.0013\n",
      "Epoch 205, Loss: 0.0012\n",
      "Epoch 206, Loss: 0.0009\n",
      "Epoch 207, Loss: 0.0009\n",
      "Epoch 208, Loss: 0.0009\n",
      "Epoch 209, Loss: 0.0011\n",
      "Epoch 210, Loss: 0.0010\n",
      "Epoch 211, Loss: 0.0008\n",
      "Epoch 212, Loss: 0.0008\n",
      "Epoch 213, Loss: 0.0012\n",
      "Epoch 214, Loss: 0.0010\n",
      "Epoch 215, Loss: 0.0010\n",
      "Epoch 216, Loss: 0.0008\n",
      "Epoch 217, Loss: 0.0009\n",
      "Epoch 218, Loss: 0.0012\n",
      "Epoch 219, Loss: 0.0013\n",
      "Epoch 220, Loss: 0.0013\n",
      "Epoch 221, Loss: 0.0010\n",
      "Epoch 222, Loss: 0.0009\n",
      "Epoch 223, Loss: 0.0011\n",
      "Epoch 224, Loss: 0.0013\n",
      "Epoch 225, Loss: 0.0010\n",
      "Epoch 226, Loss: 0.0008\n",
      "Epoch 227, Loss: 0.0008\n",
      "Epoch 228, Loss: 0.0011\n",
      "Epoch 229, Loss: 0.0010\n",
      "Epoch 230, Loss: 0.0008\n",
      "Epoch 231, Loss: 0.0008\n",
      "Epoch 232, Loss: 0.0011\n",
      "Epoch 233, Loss: 0.0009\n",
      "Epoch 234, Loss: 0.0010\n",
      "Epoch 235, Loss: 0.0012\n",
      "Epoch 236, Loss: 0.0009\n",
      "Epoch 237, Loss: 0.0008\n",
      "Epoch 238, Loss: 0.0008\n",
      "Epoch 239, Loss: 0.0007\n",
      "Epoch 240, Loss: 0.0007\n",
      "Epoch 241, Loss: 0.0008\n",
      "Epoch 242, Loss: 0.0009\n",
      "Epoch 243, Loss: 0.0010\n",
      "Epoch 244, Loss: 0.0008\n",
      "Epoch 245, Loss: 0.0008\n",
      "Epoch 246, Loss: 0.0007\n",
      "Epoch 247, Loss: 0.0011\n",
      "Epoch 248, Loss: 0.0010\n",
      "Epoch 249, Loss: 0.0008\n",
      "Epoch 250, Loss: 0.0008\n",
      "Epoch 251, Loss: 0.0012\n",
      "Epoch 252, Loss: 0.0008\n",
      "Epoch 253, Loss: 0.0007\n",
      "Epoch 254, Loss: 0.0008\n",
      "Epoch 255, Loss: 0.0008\n",
      "Epoch 256, Loss: 0.0012\n",
      "Epoch 257, Loss: 0.0011\n",
      "Epoch 258, Loss: 0.0009\n",
      "Epoch 259, Loss: 0.0008\n",
      "Epoch 260, Loss: 0.0007\n",
      "Epoch 261, Loss: 0.0006\n",
      "Epoch 262, Loss: 0.0007\n",
      "Epoch 263, Loss: 0.0007\n",
      "Epoch 264, Loss: 0.0007\n",
      "Epoch 265, Loss: 0.0009\n",
      "Epoch 266, Loss: 0.0008\n",
      "Epoch 267, Loss: 0.0007\n",
      "Epoch 268, Loss: 0.0007\n",
      "Epoch 269, Loss: 0.0010\n",
      "Epoch 270, Loss: 0.0010\n",
      "Epoch 271, Loss: 0.0012\n",
      "Epoch 272, Loss: 0.0010\n",
      "Epoch 273, Loss: 0.0007\n",
      "Epoch 274, Loss: 0.0006\n",
      "Epoch 275, Loss: 0.0007\n",
      "Epoch 276, Loss: 0.0006\n",
      "Epoch 277, Loss: 0.0005\n",
      "Epoch 278, Loss: 0.0005\n",
      "Epoch 279, Loss: 0.0006\n",
      "Epoch 280, Loss: 0.0008\n",
      "Epoch 281, Loss: 0.0007\n",
      "Epoch 282, Loss: 0.0009\n",
      "Epoch 283, Loss: 0.0008\n",
      "Epoch 284, Loss: 0.0007\n",
      "Epoch 285, Loss: 0.0008\n",
      "Epoch 286, Loss: 0.0007\n",
      "Epoch 287, Loss: 0.0009\n",
      "Epoch 288, Loss: 0.0008\n",
      "Epoch 289, Loss: 0.0007\n",
      "Epoch 290, Loss: 0.0007\n",
      "Epoch 291, Loss: 0.0007\n",
      "Epoch 292, Loss: 0.0010\n",
      "Epoch 293, Loss: 0.0009\n",
      "Epoch 294, Loss: 0.0007\n",
      "Epoch 295, Loss: 0.0006\n",
      "Epoch 296, Loss: 0.0005\n",
      "Epoch 297, Loss: 0.0006\n",
      "Epoch 298, Loss: 0.0009\n",
      "Epoch 299, Loss: 0.0008\n",
      "Epoch 300, Loss: 0.0009\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Ising Model Energy\n",
    "# ---------------------------\n",
    "def energy(config, L, plaquettes, J0=1.0, J1=0.5):\n",
    "    \"\"\"\n",
    "    Compute energy with 4-spin plaquette interaction and diagonal interactions.\n",
    "    Uses PBC and a given list of plaquettes.\n",
    "    \n",
    "    config : 1D spin array of length L*L (values ±1)\n",
    "    L      : lattice size\n",
    "    plaquettes : list of plaquettes, each = (tl, tr, br, bl) indices\n",
    "    \"\"\"\n",
    "    spins = np.array(config).flatten()\n",
    "    E = 0.0\n",
    "    for tl, tr, br, bl in plaquettes:\n",
    "        # 4-spin plaquette term\n",
    "        E += -J0 * spins[tl] * spins[tr] * spins[br] * spins[bl]\n",
    "        # diagonal Ising-like terms\n",
    "        E += J1 * (spins[tl] * spins[br] + spins[tr] * spins[bl])\n",
    "    return E\n",
    "\n",
    "\n",
    "def delta_energy(spins, flip_site, plaquettes, J0, J1):\n",
    "    \"\"\"\n",
    "    Compute ΔE for flipping one spin.\n",
    "    \n",
    "    spins      : 1D array of length L*L\n",
    "    flip_site  : index of spin to flip\n",
    "    plaquettes : list of plaquettes (indices)\n",
    "    \"\"\"\n",
    "    oldE, newE = 0.0, 0.0\n",
    "    for plaq in plaquettes:\n",
    "        if flip_site in plaq:   # only plaquettes involving this spin matter\n",
    "            tl, tr, br, bl = plaq\n",
    "            # old contribution\n",
    "            oldE += - J0 * spins[tl]*spins[tr]*spins[br]*spins[bl] + \\\n",
    "                    J1 * (spins[tl]*spins[br] +\n",
    "                          spins[tr]*spins[bl])\n",
    "            # new contribution (flip spin at flip_site)\n",
    "            flipped = spins.copy()\n",
    "            flipped[flip_site] *= -1\n",
    "            newE += - J0 * flipped[tl]*flipped[tr]*flipped[br]*flipped[bl] + \\\n",
    "                    J1 * (flipped[tl]*flipped[br] +\n",
    "                          flipped[tr]*flipped[bl])\n",
    "    return newE - oldE\n",
    "\n",
    "def generate_black_plaquettes(L, return_coords=False, periodic=True):\n",
    "    \"\"\"\n",
    "    Generate black plaquette indices for an L x L checkerboard lattice.\n",
    "\n",
    "    A plaquette is defined by 4 sites:\n",
    "      top-left  : (i, j)\n",
    "      top-right : (i, j+1)\n",
    "      bot-right : (i+1, j+1)\n",
    "      bot-left  : (i+1, j)\n",
    "\n",
    "    A plaquette is considered 'black' if (i + j) % 2 == 0 for the top-left corner.\n",
    "\n",
    "    Args:\n",
    "        L (int): lattice linear size (L x L).\n",
    "        return_coords (bool): if True, return list of plaquettes as lists of (i,j) coords.\n",
    "                              if False (default), return list of plaquettes as lists of flattened indices.\n",
    "        periodic (bool): whether to use periodic boundary conditions (wrap edges). Default True.\n",
    "\n",
    "    Returns:\n",
    "        plaquettes (list): list of plaquettes. Each plaquette is length-4 list.\n",
    "                           - If return_coords=True: elements are (i,j) tuples.\n",
    "                           - If return_coords=False: elements are linear indices idx = i*L + j.\n",
    "    \"\"\"\n",
    "    plaquettes = []\n",
    "    for i in range(L):\n",
    "        for j in range(L):\n",
    "            if (i + j) % 2 != 0:\n",
    "                continue\n",
    "            i0, j0 = i, j\n",
    "            i1, j1 = i, (j + 1) % L if periodic else j + 1\n",
    "            i2, j2 = (i + 1) % L if periodic else i + 1, (j + 1) % L if periodic else j + 1\n",
    "            i3, j3 = (i + 1) % L if periodic else i + 1, j\n",
    "\n",
    "            # if not periodic and index out of bounds, skip\n",
    "            if not periodic:\n",
    "                if i1 >= L or j1 >= L or i2 >= L or j2 >= L or i3 >= L or j3 >= L:\n",
    "                    continue\n",
    "\n",
    "            if return_coords:\n",
    "                plaquettes.append([(i0, j0), (i1, j1), (i2, j2), (i3, j3)])\n",
    "            else:\n",
    "                idx0 = i0 * L + j0\n",
    "                idx1 = i1 * L + j1\n",
    "                idx2 = i2 * L + j2\n",
    "                idx3 = i3 * L + j3\n",
    "                plaquettes.append([idx0, idx1, idx2, idx3])\n",
    "    return plaquettes\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Dataset\n",
    "# ---------------------------\n",
    "class IsingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, L=4, J0=1.0, J1=0.5, n_samples=5000, T=2.0):\n",
    "        self.L = L\n",
    "        self.N = L*L\n",
    "        self.data = []\n",
    "        plaquettes = generate_black_plaquettes(L, return_coords=False, periodic=True)\n",
    "        for _ in range(n_samples):\n",
    "            config = np.random.choice([-1, 1], size=(self.N,))\n",
    "            dEs = np.array([delta_energy(config, i, plaquettes, J0, J1) for i in range(self.N)])\n",
    "            probs = np.exp(-dEs/T)\n",
    "            probs /= probs.sum()\n",
    "            self.data.append((config, probs))\n",
    "    \n",
    "    def __len__(self): return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        config, probs = self.data[idx]\n",
    "        return torch.tensor(config, dtype=torch.float32), torch.tensor(probs, dtype=torch.float32)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Policy Network\n",
    "# ---------------------------\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, N, hidden=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(N, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.out = nn.Linear(hidden, N)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = torch.tanh(self.fc1(x))\n",
    "        h = torch.tanh(self.fc2(h))\n",
    "        return torch.softmax(self.out(h), dim=-1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Training Loop\n",
    "# ---------------------------\n",
    "def train_policy(L=4, T=2.0, epochs=10, batch_size=64, lr=1e-3):\n",
    "    dataset = IsingDataset(L=L, T=T, n_samples=5000)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = PolicyNet(N=L*L)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, target in loader:\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred.log(), target)   # minimize KL divergence\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")\n",
    "    return model\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Example Run\n",
    "# ---------------------------\n",
    "L = 4\n",
    "model = train_policy(L=L, T=1.0, epochs=300)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "10f2d293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_spin_config_checkerboard_pbc(traj[-1].reshape(L, L), L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7018d6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-16.0)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy(traj[-1].reshape(L, L), L, plaquettes, J0=1.0, J1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b71b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.2144\n",
      "Epoch 2, Loss: 2.2024\n",
      "Epoch 3, Loss: 2.1961\n",
      "Epoch 4, Loss: 2.2012\n",
      "Epoch 5, Loss: 2.1922\n",
      "Epoch 6, Loss: 2.1766\n",
      "Epoch 7, Loss: 2.1602\n",
      "Epoch 8, Loss: 2.1204\n",
      "Epoch 9, Loss: 2.0659\n",
      "Epoch 10, Loss: 1.9746\n",
      "Epoch 11, Loss: 1.8580\n",
      "Epoch 12, Loss: 1.7249\n",
      "Epoch 13, Loss: 1.5963\n",
      "Epoch 14, Loss: 1.4767\n",
      "Epoch 15, Loss: 1.3774\n",
      "Epoch 16, Loss: 1.3035\n",
      "Epoch 17, Loss: 1.2335\n",
      "Epoch 18, Loss: 1.1889\n",
      "Epoch 19, Loss: 1.1431\n",
      "Epoch 20, Loss: 1.1142\n",
      "Epoch 21, Loss: 1.0966\n",
      "Epoch 22, Loss: 1.0670\n",
      "Epoch 23, Loss: 1.0336\n",
      "Epoch 24, Loss: 1.0038\n",
      "Epoch 25, Loss: 0.9863\n",
      "Epoch 26, Loss: 0.9549\n",
      "Epoch 27, Loss: 0.9187\n",
      "Epoch 28, Loss: 0.8826\n",
      "Epoch 29, Loss: 0.8476\n",
      "Epoch 30, Loss: 0.8079\n",
      "Epoch 31, Loss: 0.7788\n",
      "Epoch 32, Loss: 0.7450\n",
      "Epoch 33, Loss: 0.7106\n",
      "Epoch 34, Loss: 0.6743\n",
      "Epoch 35, Loss: 0.6430\n",
      "Epoch 36, Loss: 0.6129\n",
      "Epoch 37, Loss: 0.5772\n",
      "Epoch 38, Loss: 0.5511\n",
      "Epoch 39, Loss: 0.5204\n",
      "Epoch 40, Loss: 0.4975\n",
      "Epoch 41, Loss: 0.4706\n",
      "Epoch 42, Loss: 0.4456\n",
      "Epoch 43, Loss: 0.4246\n",
      "Epoch 44, Loss: 0.4021\n",
      "Epoch 45, Loss: 0.3828\n",
      "Epoch 46, Loss: 0.3614\n",
      "Epoch 47, Loss: 0.3485\n",
      "Epoch 48, Loss: 0.3285\n",
      "Epoch 49, Loss: 0.3094\n",
      "Epoch 50, Loss: 0.2978\n",
      "T=0.1, <E> = -8.0000 ± 0.0000\n",
      "Epoch 1, Loss: 2.0499\n",
      "Epoch 2, Loss: 2.0374\n",
      "Epoch 3, Loss: 2.0318\n",
      "Epoch 4, Loss: 2.0233\n",
      "Epoch 5, Loss: 2.0151\n",
      "Epoch 6, Loss: 2.0076\n",
      "Epoch 7, Loss: 1.9851\n",
      "Epoch 8, Loss: 1.9428\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[174], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m energies \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m T \u001b[38;5;129;01min\u001b[39;00m betas:   \u001b[38;5;66;03m# note: here using T = 1/β if you want inverse\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     model \u001b[38;5;241m=\u001b[39m train_policy(L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, T\u001b[38;5;241m=\u001b[39mT, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)   \u001b[38;5;66;03m# retrain at each T\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     meanE, errE \u001b[38;5;241m=\u001b[39m sample_average_energy(model, L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, T\u001b[38;5;241m=\u001b[39mT, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, <E> = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeanE\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merrE\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[161], line 150\u001b[0m, in \u001b[0;36mtrain_policy\u001b[1;34m(L, T, epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m    149\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, target \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m    151\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m    152\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(pred\u001b[38;5;241m.\u001b[39mlog(), target)   \u001b[38;5;66;03m# minimize KL divergence\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Conda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Conda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Conda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[161], line 120\u001b[0m, in \u001b[0;36mIsingDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m    119\u001b[0m     config, probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(config, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), torch\u001b[38;5;241m.\u001b[39mtensor(probs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def sample_average_energy(model, L=4, T=1.0, n_steps=5000, burn_in=1000, J0=1.0, J1=0.5):\n",
    "    \"\"\"\n",
    "    Use trained policy network to sample configurations and compute average energy.\n",
    "    \n",
    "    model   : trained PolicyNet\n",
    "    L       : lattice size\n",
    "    T       : temperature (used only in energy, not directly in model)\n",
    "    n_steps : total MC steps\n",
    "    burn_in : number of equilibration steps to discard\n",
    "    \"\"\"\n",
    "    N = L * L\n",
    "    plaquettes = generate_black_plaquettes(L)\n",
    "    \n",
    "    # random initial configuration\n",
    "    config = np.random.choice([-1, 1], size=(N,))\n",
    "    \n",
    "    energies = []\n",
    "    for step in range(n_steps):\n",
    "        x = torch.tensor(config, dtype=torch.float32).unsqueeze(0)  # shape (1, N)\n",
    "        probs = model(x).detach().numpy().flatten()\n",
    "        \n",
    "        # propose a spin flip according to ML policy\n",
    "        flip_site = np.random.choice(N, p=probs)\n",
    "        \n",
    "        # compute ΔE\n",
    "        E_old = energy(config, L, plaquettes, J0, J1)\n",
    "        config_new = config.copy()\n",
    "        config_new[flip_site] *= -1\n",
    "        E_new = energy(config_new, L, plaquettes, J0, J1)\n",
    "        dE = E_new - E_old\n",
    "        \n",
    "        # Metropolis acceptance with Boltzmann weight\n",
    "        if dE < 0 or np.random.rand() < np.exp(-dE / T):\n",
    "            config = config_new  # accept move\n",
    "        \n",
    "        # measure only after burn-in\n",
    "        if step >= burn_in:\n",
    "            E = energy(config, L, plaquettes, J0, J1)\n",
    "            energies.append(E)\n",
    "\n",
    "    \n",
    "    return np.mean(energies), np.std(energies)/np.sqrt(len(energies))  # mean ± error\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Example: compute ⟨E⟩ at different T\n",
    "# ---------------------------\n",
    "betas = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "energies = []\n",
    "\n",
    "for T in betas:   # note: here using T = 1/β if you want inverse\n",
    "    model = train_policy(L=4, T=T, epochs=50)   # retrain at each T\n",
    "    meanE, errE = sample_average_energy(model, L=4, T=T, n_steps=2000)\n",
    "    print(f\"T={T}, <E> = {meanE:.4f} ± {errE:.4f}\")\n",
    "    energies.append(meanE)\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(betas, energies, \"o-\")\n",
    "plt.xlabel(\"T\")\n",
    "plt.ylabel(\"<E>\")\n",
    "plt.title(\"Average Energy vs Temperature (ML policy sampling)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe83f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ml_mh_sampler(model, L, plaquettes, J0, J1, T,\n",
    "                  start_config=None, burn_in=2000, n_samples=5000, thinning=1, rng=None):\n",
    "    \"\"\"\n",
    "    model: policy network mapping config -> probs over spin sites\n",
    "    returns: samples (array n_samples x N), energies list\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    N = L*L\n",
    "    if start_config is None:\n",
    "        config = rng.choice([-1,1], size=(N,))\n",
    "    else:\n",
    "        config = start_config.copy()\n",
    "\n",
    "    def energy_fn(cfg): return energy(cfg, L, plaquettes, J0=J0, J1=J1)\n",
    "\n",
    "    # burn-in\n",
    "    for _ in range(burn_in):\n",
    "        x = torch.tensor(config, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = model(x).detach().cpu().numpy().ravel()\n",
    "        probs = np.clip(probs, 1e-12, 1-1e-12)\n",
    "        probs /= probs.sum()\n",
    "        i = rng.choice(N, p=probs)\n",
    "        prop = config.copy(); prop[i] *= -1\n",
    "        dE = delta_energy(config, i, plaquettes, J0, J1)\n",
    "        if rng.random() < np.exp(-dE / T):\n",
    "            config = prop\n",
    "\n",
    "    # sampling\n",
    "    samples = []\n",
    "    energies = []\n",
    "    steps = 0\n",
    "    while len(samples) < n_samples:\n",
    "        x = torch.tensor(config, dtype=torch.float32).unsqueeze(0)\n",
    "        probs = model(x).detach().cpu().numpy().ravel()\n",
    "        probs = np.clip(probs, 1e-12, 1-1e-12)\n",
    "        probs /= probs.sum()\n",
    "        i = rng.choice(N, p=probs)\n",
    "        prop = config.copy(); prop[i] *= -1\n",
    "        dE = delta_energy(config, i, plaquettes, J0, J1)\n",
    "        if rng.random() < np.exp(-dE / T):\n",
    "            config = prop\n",
    "        steps += 1\n",
    "        if steps % thinning == 0:\n",
    "            samples.append(config.copy())\n",
    "            energies.append(energy_fn(config))\n",
    "    return np.array(samples), np.array(energies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7eb3eb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.5105\n",
      "Epoch 2, Loss: 1.5014\n",
      "Epoch 3, Loss: 1.4997\n",
      "Epoch 4, Loss: 1.4937\n",
      "Epoch 5, Loss: 1.4883\n",
      "Epoch 6, Loss: 1.4779\n",
      "Epoch 7, Loss: 1.4620\n",
      "Epoch 8, Loss: 1.4277\n",
      "Epoch 9, Loss: 1.3776\n",
      "Epoch 10, Loss: 1.2982\n",
      "Epoch 11, Loss: 1.1981\n",
      "Epoch 12, Loss: 1.1070\n",
      "Epoch 13, Loss: 1.0237\n",
      "Epoch 14, Loss: 0.9602\n",
      "Epoch 15, Loss: 0.9069\n",
      "Epoch 16, Loss: 0.8666\n",
      "Epoch 17, Loss: 0.8242\n",
      "Epoch 18, Loss: 0.7838\n",
      "Epoch 19, Loss: 0.7431\n",
      "Epoch 20, Loss: 0.7009\n",
      "Epoch 21, Loss: 0.6549\n",
      "Epoch 22, Loss: 0.6041\n",
      "Epoch 23, Loss: 0.5520\n",
      "Epoch 24, Loss: 0.4990\n",
      "Epoch 25, Loss: 0.4506\n",
      "Epoch 26, Loss: 0.4019\n",
      "Epoch 27, Loss: 0.3577\n",
      "Epoch 28, Loss: 0.3163\n",
      "Epoch 29, Loss: 0.2822\n",
      "Epoch 30, Loss: 0.2521\n",
      "Epoch 31, Loss: 0.2229\n",
      "Epoch 32, Loss: 0.1990\n",
      "Epoch 33, Loss: 0.1785\n",
      "Epoch 34, Loss: 0.1599\n",
      "Epoch 35, Loss: 0.1437\n",
      "Epoch 36, Loss: 0.1287\n",
      "Epoch 37, Loss: 0.1166\n",
      "Epoch 38, Loss: 0.1056\n",
      "Epoch 39, Loss: 0.0948\n",
      "Epoch 40, Loss: 0.0854\n",
      "Epoch 41, Loss: 0.0773\n",
      "Epoch 42, Loss: 0.0699\n",
      "Epoch 43, Loss: 0.0637\n",
      "Epoch 44, Loss: 0.0574\n",
      "Epoch 45, Loss: 0.0520\n",
      "Epoch 46, Loss: 0.0482\n",
      "Epoch 47, Loss: 0.0437\n",
      "Epoch 48, Loss: 0.0395\n",
      "Epoch 49, Loss: 0.0355\n",
      "Epoch 50, Loss: 0.0323\n",
      "Epoch 51, Loss: 0.0300\n",
      "Epoch 52, Loss: 0.0278\n",
      "Epoch 53, Loss: 0.0257\n",
      "Epoch 54, Loss: 0.0237\n",
      "Epoch 55, Loss: 0.0220\n",
      "Epoch 56, Loss: 0.0201\n",
      "Epoch 57, Loss: 0.0185\n",
      "Epoch 58, Loss: 0.0175\n",
      "Epoch 59, Loss: 0.0162\n",
      "Epoch 60, Loss: 0.0152\n",
      "Epoch 61, Loss: 0.0146\n",
      "Epoch 62, Loss: 0.0134\n",
      "Epoch 63, Loss: 0.0130\n",
      "Epoch 64, Loss: 0.0123\n",
      "Epoch 65, Loss: 0.0120\n",
      "Epoch 66, Loss: 0.0111\n",
      "Epoch 67, Loss: 0.0105\n",
      "Epoch 68, Loss: 0.0100\n",
      "Epoch 69, Loss: 0.0094\n",
      "Epoch 70, Loss: 0.0090\n",
      "Epoch 71, Loss: 0.0086\n",
      "Epoch 72, Loss: 0.0082\n",
      "Epoch 73, Loss: 0.0080\n",
      "Epoch 74, Loss: 0.0076\n",
      "Epoch 75, Loss: 0.0074\n",
      "Epoch 76, Loss: 0.0071\n",
      "Epoch 77, Loss: 0.0068\n",
      "Epoch 78, Loss: 0.0064\n",
      "Epoch 79, Loss: 0.0063\n",
      "Epoch 80, Loss: 0.0062\n",
      "Epoch 81, Loss: 0.0060\n",
      "Epoch 82, Loss: 0.0057\n",
      "Epoch 83, Loss: 0.0055\n",
      "Epoch 84, Loss: 0.0051\n",
      "Epoch 85, Loss: 0.0052\n",
      "Epoch 86, Loss: 0.0051\n",
      "Epoch 87, Loss: 0.0048\n",
      "Epoch 88, Loss: 0.0048\n",
      "Epoch 89, Loss: 0.0046\n",
      "Epoch 90, Loss: 0.0045\n",
      "Epoch 91, Loss: 0.0042\n",
      "Epoch 92, Loss: 0.0041\n",
      "Epoch 93, Loss: 0.0044\n",
      "Epoch 94, Loss: 0.0041\n",
      "Epoch 95, Loss: 0.0038\n",
      "Epoch 96, Loss: 0.0039\n",
      "Epoch 97, Loss: 0.0038\n",
      "Epoch 98, Loss: 0.0037\n",
      "Epoch 99, Loss: 0.0035\n",
      "Epoch 100, Loss: 0.0036\n",
      "Epoch 101, Loss: 0.0034\n",
      "Epoch 102, Loss: 0.0035\n",
      "Epoch 103, Loss: 0.0034\n",
      "Epoch 104, Loss: 0.0031\n",
      "Epoch 105, Loss: 0.0033\n",
      "Epoch 106, Loss: 0.0032\n",
      "Epoch 107, Loss: 0.0031\n",
      "Epoch 108, Loss: 0.0031\n",
      "Epoch 109, Loss: 0.0028\n",
      "Epoch 110, Loss: 0.0029\n",
      "Epoch 111, Loss: 0.0029\n",
      "Epoch 112, Loss: 0.0028\n",
      "Epoch 113, Loss: 0.0027\n",
      "Epoch 114, Loss: 0.0030\n",
      "Epoch 115, Loss: 0.0029\n",
      "Epoch 116, Loss: 0.0029\n",
      "Epoch 117, Loss: 0.0028\n",
      "Epoch 118, Loss: 0.0031\n",
      "Epoch 119, Loss: 0.0029\n",
      "Epoch 120, Loss: 0.0024\n",
      "Epoch 121, Loss: 0.0027\n",
      "Epoch 122, Loss: 0.0025\n",
      "Epoch 123, Loss: 0.0025\n",
      "Epoch 124, Loss: 0.0024\n",
      "Epoch 125, Loss: 0.0024\n",
      "Epoch 126, Loss: 0.0022\n",
      "Epoch 127, Loss: 0.0023\n",
      "Epoch 128, Loss: 0.0023\n",
      "Epoch 129, Loss: 0.0021\n",
      "Epoch 130, Loss: 0.0020\n",
      "Epoch 131, Loss: 0.0020\n",
      "Epoch 132, Loss: 0.0020\n",
      "Epoch 133, Loss: 0.0022\n",
      "Epoch 134, Loss: 0.0021\n",
      "Epoch 135, Loss: 0.0024\n",
      "Epoch 136, Loss: 0.0020\n",
      "Epoch 137, Loss: 0.0021\n",
      "Epoch 138, Loss: 0.0020\n",
      "Epoch 139, Loss: 0.0019\n",
      "Epoch 140, Loss: 0.0022\n",
      "Epoch 141, Loss: 0.0020\n",
      "Epoch 142, Loss: 0.0020\n",
      "Epoch 143, Loss: 0.0021\n",
      "Epoch 144, Loss: 0.0018\n",
      "Epoch 145, Loss: 0.0017\n",
      "Epoch 146, Loss: 0.0017\n",
      "Epoch 147, Loss: 0.0017\n",
      "Epoch 148, Loss: 0.0019\n",
      "Epoch 149, Loss: 0.0020\n",
      "Epoch 150, Loss: 0.0021\n",
      "Epoch 151, Loss: 0.0019\n",
      "Epoch 152, Loss: 0.0017\n",
      "Epoch 153, Loss: 0.0018\n",
      "Epoch 154, Loss: 0.0017\n",
      "Epoch 155, Loss: 0.0017\n",
      "Epoch 156, Loss: 0.0018\n",
      "Epoch 157, Loss: 0.0018\n",
      "Epoch 158, Loss: 0.0016\n",
      "Epoch 159, Loss: 0.0017\n",
      "Epoch 160, Loss: 0.0018\n",
      "Epoch 161, Loss: 0.0017\n",
      "Epoch 162, Loss: 0.0024\n",
      "Epoch 163, Loss: 0.0017\n",
      "Epoch 164, Loss: 0.0014\n",
      "Epoch 165, Loss: 0.0015\n",
      "Epoch 166, Loss: 0.0015\n",
      "Epoch 167, Loss: 0.0016\n",
      "Epoch 168, Loss: 0.0014\n",
      "Epoch 169, Loss: 0.0017\n",
      "Epoch 170, Loss: 0.0013\n",
      "Epoch 171, Loss: 0.0013\n",
      "Epoch 172, Loss: 0.0013\n",
      "Epoch 173, Loss: 0.0013\n",
      "Epoch 174, Loss: 0.0015\n",
      "Epoch 175, Loss: 0.0018\n",
      "Epoch 176, Loss: 0.0025\n",
      "Epoch 177, Loss: 0.0014\n",
      "Epoch 178, Loss: 0.0014\n",
      "Epoch 179, Loss: 0.0012\n",
      "Epoch 180, Loss: 0.0015\n",
      "Epoch 181, Loss: 0.0016\n",
      "Epoch 182, Loss: 0.0013\n",
      "Epoch 183, Loss: 0.0013\n",
      "Epoch 184, Loss: 0.0013\n",
      "Epoch 185, Loss: 0.0014\n",
      "Epoch 186, Loss: 0.0014\n",
      "Epoch 187, Loss: 0.0015\n",
      "Epoch 188, Loss: 0.0013\n",
      "Epoch 189, Loss: 0.0014\n",
      "Epoch 190, Loss: 0.0014\n",
      "Epoch 191, Loss: 0.0015\n",
      "Epoch 192, Loss: 0.0015\n",
      "Epoch 193, Loss: 0.0013\n",
      "Epoch 194, Loss: 0.0016\n",
      "Epoch 195, Loss: 0.0013\n",
      "Epoch 196, Loss: 0.0011\n",
      "Epoch 197, Loss: 0.0011\n",
      "Epoch 198, Loss: 0.0013\n",
      "Epoch 199, Loss: 0.0012\n",
      "Epoch 200, Loss: 0.0013\n"
     ]
    }
   ],
   "source": [
    "model = train_policy(L=4, T=1.0, epochs=200)   # retrain at each T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b1ef9825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⟨E⟩ = -0.999250 ± 0.001895\n"
     ]
    }
   ],
   "source": [
    "samples, energies = ml_mh_sampler(model, L, plaquettes, J0, J1, 1.0,\n",
    "                                  burn_in=2000, n_samples=20000, thinning=5)\n",
    "E_mean = energies.mean()/L/L\n",
    "E_std = energies.std() / np.sqrt(len(energies))\n",
    "print(\"⟨E⟩ = {:.6f} ± {:.6f}\".format(E_mean, E_std))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd113b64",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa70240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plaquette:\n",
    "    def __init__(self, sites, coord=None):\n",
    "        self.sites = sites   # list of 4 site indices\n",
    "        self.coord = coord   # (x, y) coordinate of top-left corner\n",
    "\n",
    "def generate_black_plaquette_indices(Lx, Ly):\n",
    "    \"\"\"\n",
    "    Generate black plaquette site indices with PBC.\n",
    "    Each plaquette is tagged by its (x,y) coordinate.\n",
    "    \"\"\"\n",
    "    black_plaquette_indices = []\n",
    "\n",
    "    def idx(x, y):\n",
    "        return x * Ly + y\n",
    "\n",
    "    for x in range(Lx):\n",
    "        for y in range(Ly):\n",
    "            if (x + y) % 2 == 0:  # black plaquette condition\n",
    "                tl = idx(x, y)\n",
    "                tr = idx(x, (y + 1) % Ly)\n",
    "                br = idx((x + 1) % Lx, (y + 1) % Ly)\n",
    "                bl = idx((x + 1) % Lx, y)\n",
    "                plaquette = Plaquette([tl, tr, br, bl], coord=(x, y))\n",
    "                black_plaquette_indices.append(plaquette)\n",
    "\n",
    "    return black_plaquette_indices\n",
    "\n",
    "\n",
    "def get_black_plaquette_neighbors(plaquette, Lx, Ly):\n",
    "    \"\"\"\n",
    "    Given a black plaquette, return the 4 neighboring plaquettes\n",
    "    (up, down, left, right) using PBC.\n",
    "    \"\"\"\n",
    "    x, y = plaquette.coord\n",
    "    neighbors = []\n",
    "\n",
    "    # Up, Down, Left, Right coordinates\n",
    "    neighbor_coords = [\n",
    "        ((x - 2) % Lx, y),       # up (step 2 since checkerboard spacing)\n",
    "        ((x + 2) % Lx, y),       # down\n",
    "        (x, (y - 2) % Ly),       # left\n",
    "        (x, (y + 2) % Ly)        # right\n",
    "    ]\n",
    "\n",
    "    # Convert coords back to Plaquette objects\n",
    "    for nx, ny in neighbor_coords:\n",
    "        # recompute sites of neighbor\n",
    "        tl = (nx % Lx) * Ly + (ny % Ly)\n",
    "        tr = (nx % Lx) * Ly + ((ny + 1) % Ly)\n",
    "        br = ((nx + 1) % Lx) * Ly + ((ny + 1) % Ly)\n",
    "        bl = ((nx + 1) % Lx) * Ly + (ny % Ly)\n",
    "        neighbors.append(Plaquette([tl, tr, br, bl], coord=(nx, ny)))\n",
    "\n",
    "    return neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297df2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black plaquette 0: [0, 1, 5, 4]\n",
      "Black plaquette 1: [2, 3, 7, 6]\n",
      "Black plaquette 2: [5, 6, 10, 9]\n",
      "Black plaquette 3: [7, 4, 8, 11]\n",
      "Black plaquette 4: [8, 9, 13, 12]\n",
      "Black plaquette 5: [10, 11, 15, 14]\n",
      "Black plaquette 6: [13, 14, 2, 1]\n",
      "Black plaquette 7: [15, 12, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "def list_black_plaquettes(Lx, Ly):\n",
    "    def idx(x, y):\n",
    "        return x * Ly + y\n",
    "\n",
    "    black_plaquettes = []\n",
    "\n",
    "    for x in range(Lx):\n",
    "        for y in range(Ly):\n",
    "            if (x + y) % 2 == 0:  # black plaquette condition\n",
    "                tl = idx(x, y)\n",
    "                tr = idx(x, (y + 1) % Ly)\n",
    "                br = idx((x + 1) % Lx, (y + 1) % Ly)\n",
    "                bl = idx((x + 1) % Lx, y)\n",
    "                plaquette = [tl, tr, br, bl]\n",
    "                black_plaquettes.append(plaquette)\n",
    "\n",
    "    return black_plaquettes\n",
    "\n",
    "\n",
    "# Example: 4x4 lattice\n",
    "plaquettes = list_black_plaquettes(4, 4)\n",
    "for i, p in enumerate(plaquettes):\n",
    "    print(f\"Black plaquette {i}: {p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed1438",
   "metadata": {},
   "source": [
    "# Autoregressive models for QMC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2e989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f106a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples[0]:\n",
      " [[-1 -1 -1 -1]\n",
      " [-1 -1 -1  1]\n",
      " [-1  1 -1 -1]\n",
      " [ 1 -1  1  1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def random_spins(n_samples: int, L: int, seed: int | None = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate random spin configurations on an L x L lattice.\n",
    "    If seed is None, uses current time to generate a different random state each run.\n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        seed = int(time.time() * 1000) % 2**32  # current time in milliseconds\n",
    "    rng = np.random.default_rng(seed)\n",
    "    configs = rng.choice([-1, 1], size=(n_samples, L, L))\n",
    "    return configs\n",
    "\n",
    "# -- test snippet --\n",
    "if __name__ == \"__main__\":\n",
    "    samples = random_spins(2, 4)  # different each run\n",
    "    print(\"samples[0]:\\n\", samples[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spins = random_spins(1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd71b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def spins_to_tensor(spins: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert NumPy spin array {-1, +1} to PyTorch tensor {0,1} for network input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spins : np.ndarray, shape (batch, L, L)\n",
    "        Spin configurations with values -1 or +1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor : torch.Tensor, shape (batch, 1, L, L), dtype=torch.float32\n",
    "        Spins mapped to {0,1}, ready for neural network.\n",
    "    \"\"\"\n",
    "    tensor = torch.from_numpy((spins + 1) // 2).float()  # -1 ->0, +1->1\n",
    "    return tensor.unsqueeze(1)  # add channel dimension\n",
    "\n",
    "def tensor_to_spins(tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert PyTorch tensor {0,1} back to spins {-1,+1}.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor : torch.Tensor, shape (batch, 1, L, L)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    spins : np.ndarray, shape (batch, L, L)\n",
    "    \"\"\"\n",
    "    spins = tensor.squeeze(1).cpu().numpy()\n",
    "    return 2 * spins - 1  # 0->-1, 1->+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f2dec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 1., 0.],\n",
       "          [0., 1., 0., 0.],\n",
       "          [0., 1., 0., 1.],\n",
       "          [0., 0., 1., 0.]]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spins_to_tensor(spins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d4e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ARWavefunction(nn.Module):\n",
    "    def __init__(self, L, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Simple autoregressive network for square lattice spins.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        L : int\n",
    "            Lattice side length.\n",
    "        hidden_dim : int\n",
    "            Number of hidden neurons in the fully connected layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "        self.N = L * L  # total number of spins\n",
    "\n",
    "        # input: flattened lattice (batch, N)\n",
    "        self.fc1 = nn.Linear(self.N, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)  # output: log-probs for spin ±1\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: compute log-probabilities for next spin.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor, shape (batch, 1, L, L)\n",
    "            Input spin configuration, {0,1}.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        log_probs : torch.Tensor, shape (batch, 2)\n",
    "            Log-probabilities for spin -1 or +1 at next site.\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        x_flat = x.view(batch_size, -1)  # flatten lattice\n",
    "        h = torch.tanh(self.fc1(x_flat))\n",
    "        out = self.fc2(h)\n",
    "        log_probs = F.log_softmax(out, dim=-1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213dbfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs shape: torch.Size([10, 2])\n",
      "log_probs:\n",
      " tensor([[-0.6803, -0.7061],\n",
      "        [-0.6024, -0.7929],\n",
      "        [-0.9105, -0.5147],\n",
      "        [-0.5975, -0.7989],\n",
      "        [-0.7216, -0.6655],\n",
      "        [-0.7052, -0.6813],\n",
      "        [-0.5163, -0.9081],\n",
      "        [-0.6436, -0.7453],\n",
      "        [-0.6013, -0.7944],\n",
      "        [-0.4131, -1.0835]], grad_fn=<LogSoftmaxBackward0>)\n",
      "Sampled spins (0 or 1):\n",
      " [1 0 0 0 1 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    L = 4\n",
    "    batch_size = 10\n",
    "\n",
    "    # generate random spins\n",
    "    spins = random_spins(batch_size, L)\n",
    "    tensor = spins_to_tensor(spins)\n",
    "\n",
    "    # initialize AR network\n",
    "    ar_net = ARWavefunction(L=L, hidden_dim=64)\n",
    "\n",
    "    # forward pass\n",
    "    log_probs = ar_net(tensor)\n",
    "    print(\"log_probs shape:\", log_probs.shape)  # should be (batch, 2)\n",
    "    print(\"log_probs:\\n\", log_probs)\n",
    "    probs = torch.exp(log_probs)        # convert log-prob -> probability\n",
    "    spin_sample = torch.multinomial(probs, 1)  # pick -1 or +1\n",
    "    print(\"Sampled spins (0 or 1):\\n\", spin_sample.squeeze().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ar(model: nn.Module, L: int, n_samples: int = 1, device='cpu') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate lattice configurations autoregressively. Keep graph for backprop.\n",
    "    \"\"\"\n",
    "    N = L * L\n",
    "    samples = torch.zeros((n_samples, N), device=device)  # start empty lattice\n",
    "\n",
    "    model.eval()  # you can also keep train mode; up to you\n",
    "    for i in range(N):\n",
    "        x_in = samples.clone().view(n_samples, 1, L, L)\n",
    "        log_probs = model(x_in)  # shape (batch, 2)\n",
    "        probs = torch.exp(log_probs)\n",
    "\n",
    "        # Use differentiable sampling: here we use categorical sampling via straight-through trick\n",
    "        # 1. Sample one-hot\n",
    "        sampled_onehot = torch.multinomial(probs, num_samples=1)\n",
    "        # 2. Convert to float and straight-through\n",
    "        spin_sample = torch.zeros_like(probs)\n",
    "        spin_sample.scatter_(1, sampled_onehot, 1.0)\n",
    "        # 3. Take argmax as 0/1 for lattice, but keep gradient\n",
    "        samples[:, i] = spin_sample[:, 1]  # +1 spin probability\n",
    "    samples = samples.view(n_samples, 1, L, L)\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ff9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled tensor shape: torch.Size([2, 1, 4, 4])\n",
      "Sampled tensor[0,0]:\n",
      " tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 1., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [0., 1., 1., 1.]])\n",
      "Sampled spins[0]:\n",
      " [[ 1. -1. -1. -1.]\n",
      " [-1.  1.  1.  1.]\n",
      " [-1. -1.  1.  1.]\n",
      " [-1.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    L = 4\n",
    "    ar_net = ARWavefunction(L=L, hidden_dim=64)\n",
    "\n",
    "    sampled = sample_ar(ar_net, L, n_samples=2)\n",
    "    print(\"Sampled tensor shape:\", sampled.shape)\n",
    "    print(\"Sampled tensor[0,0]:\\n\", sampled[0,0])\n",
    "\n",
    "    # Convert back to spins {-1,+1} for energy computation\n",
    "    spins = tensor_to_spins(sampled)\n",
    "    print(\"Sampled spins[0]:\\n\", spins[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_energy(spins: torch.Tensor, J: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute nearest-neighbor Ising energy for batch of lattices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spins : torch.Tensor, shape (batch, 1, L, L), values {-1,+1}\n",
    "    J : float\n",
    "        Coupling constant.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    energies : torch.Tensor, shape (batch,)\n",
    "    \"\"\"\n",
    "    batch, _, L, _ = spins.shape\n",
    "    spins = spins.squeeze(1)  # shape (batch, L, L)\n",
    "    E = torch.zeros(batch, device=spins.device)\n",
    "\n",
    "    # horizontal neighbors\n",
    "    E -= J * (spins * torch.roll(spins, shifts=-1, dims=2)).sum(dim=(1,2))\n",
    "    # vertical neighbors\n",
    "    E -= J * (spins * torch.roll(spins, shifts=-1, dims=1)).sum(dim=(1,2))\n",
    "    return E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e355fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_prob(model: nn.Module, samples: torch.Tensor, L: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute log-probabilities of full lattice configurations under AR network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        ARWavefunction network\n",
    "    samples : torch.Tensor, shape (batch, 1, L, L), values {0,1}\n",
    "    L : int\n",
    "        Lattice size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    log_probs : torch.Tensor, shape (batch,)\n",
    "        Sum of log-probabilities for each configuration\n",
    "    \"\"\"\n",
    "    batch_size = samples.shape[0]\n",
    "    N = L*L\n",
    "    log_probs = torch.zeros(batch_size, device=samples.device)\n",
    "\n",
    "    model.eval()\n",
    "    for i in range(N):\n",
    "        x_in = samples.clone().view(batch_size, 1, L, L)\n",
    "        out = model(x_in)                  # log_softmax output, shape (batch,2)\n",
    "        idx = samples.view(batch_size, N)[:, i].long()  # 0 or 1 for spin\n",
    "        log_probs += out.gather(1, idx.unsqueeze(1)).squeeze(1)\n",
    "    return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ce76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vmc_logprob(model: nn.Module, L: int, n_epochs: int = 50, batch_size: int = 32,\n",
    "                       lr: float = 1e-3, device='cpu', J: float = 1.0):\n",
    "    \"\"\"\n",
    "    VMC training using log-derivative trick (REINFORCE).\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "\n",
    "        # Step 1: sample lattice configurations (discrete spins 0/1)\n",
    "        samples = sample_ar(model, L, n_samples=batch_size, device=device)\n",
    "\n",
    "        # Step 2: convert to spins {-1,+1} for energy computation\n",
    "        spins = 2*samples - 1\n",
    "        spins = spins.float()\n",
    "\n",
    "        # Step 3: compute energies\n",
    "        E_loc = compute_energy(spins, J=J)  # shape (batch,)\n",
    "\n",
    "        # Step 4: compute log-probabilities\n",
    "        logP = compute_log_prob(model, samples, L)  # shape (batch,)\n",
    "\n",
    "        # Step 5: REINFORCE loss\n",
    "        E_mean = E_loc.mean()\n",
    "        loss = ((E_loc - E_mean) * logP ).mean()\n",
    "\n",
    "        # Step 6: update network\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:03d}: mean energy = {E_loc.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1285334d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: mean energy = -0.7422\n",
      "Epoch 005: mean energy = -1.2422\n",
      "Epoch 010: mean energy = -3.3086\n",
      "Epoch 015: mean energy = -6.1055\n",
      "Epoch 020: mean energy = -9.0781\n",
      "Epoch 025: mean energy = -11.8945\n",
      "Epoch 030: mean energy = -15.2227\n",
      "Epoch 035: mean energy = -17.4414\n",
      "Epoch 040: mean energy = -19.9453\n",
      "Epoch 045: mean energy = -21.9492\n",
      "Epoch 050: mean energy = -23.8086\n",
      "Epoch 055: mean energy = -25.1992\n",
      "Epoch 060: mean energy = -26.5625\n",
      "Epoch 065: mean energy = -27.4180\n",
      "Epoch 070: mean energy = -28.4727\n",
      "Epoch 075: mean energy = -28.6875\n",
      "Epoch 080: mean energy = -29.3789\n",
      "Epoch 085: mean energy = -29.8242\n",
      "Epoch 090: mean energy = -30.1055\n",
      "Epoch 095: mean energy = -30.4453\n",
      "Epoch 100: mean energy = -30.4805\n",
      "Epoch 105: mean energy = -30.6562\n",
      "Epoch 110: mean energy = -31.0625\n",
      "Epoch 115: mean energy = -30.8906\n",
      "Epoch 120: mean energy = -31.1211\n",
      "Epoch 125: mean energy = -31.1953\n",
      "Epoch 130: mean energy = -31.2422\n",
      "Epoch 135: mean energy = -31.3203\n",
      "Epoch 140: mean energy = -31.4609\n",
      "Epoch 145: mean energy = -31.4609\n",
      "Epoch 150: mean energy = -31.4609\n",
      "Epoch 155: mean energy = -31.6016\n",
      "Epoch 160: mean energy = -31.4805\n",
      "Epoch 165: mean energy = -31.6016\n",
      "Epoch 170: mean energy = -31.5859\n",
      "Epoch 175: mean energy = -31.6328\n",
      "Epoch 180: mean energy = -31.6406\n",
      "Epoch 185: mean energy = -31.7031\n",
      "Epoch 190: mean energy = -31.6562\n",
      "Epoch 195: mean energy = -31.6719\n",
      "Epoch 200: mean energy = -31.7266\n",
      "Epoch 205: mean energy = -31.7812\n",
      "Epoch 210: mean energy = -31.7188\n",
      "Epoch 215: mean energy = -31.7344\n",
      "Epoch 220: mean energy = -31.7148\n",
      "Epoch 225: mean energy = -31.7734\n",
      "Epoch 230: mean energy = -31.7734\n",
      "Epoch 235: mean energy = -31.7812\n",
      "Epoch 240: mean energy = -31.7266\n",
      "Epoch 245: mean energy = -31.7656\n",
      "Epoch 250: mean energy = -31.7734\n",
      "Epoch 255: mean energy = -31.8125\n",
      "Epoch 260: mean energy = -31.8125\n",
      "Epoch 265: mean energy = -31.7578\n",
      "Epoch 270: mean energy = -31.8672\n",
      "Epoch 275: mean energy = -31.8672\n",
      "Epoch 280: mean energy = -31.8672\n",
      "Epoch 285: mean energy = -31.8984\n",
      "Epoch 290: mean energy = -31.8281\n",
      "Epoch 295: mean energy = -31.8594\n",
      "Epoch 300: mean energy = -31.9297\n",
      "Epoch 305: mean energy = -31.8594\n",
      "Epoch 310: mean energy = -31.8750\n",
      "Epoch 315: mean energy = -31.8594\n",
      "Epoch 320: mean energy = -31.9141\n",
      "Epoch 325: mean energy = -31.8984\n",
      "Epoch 330: mean energy = -31.9609\n",
      "Epoch 335: mean energy = -31.8516\n",
      "Epoch 340: mean energy = -31.9219\n",
      "Epoch 345: mean energy = -31.9062\n",
      "Epoch 350: mean energy = -31.9219\n",
      "Epoch 355: mean energy = -31.9297\n",
      "Epoch 360: mean energy = -31.8906\n",
      "Epoch 365: mean energy = -31.9062\n",
      "Epoch 370: mean energy = -31.8828\n",
      "Epoch 375: mean energy = -31.8828\n",
      "Epoch 380: mean energy = -31.8750\n",
      "Epoch 385: mean energy = -31.8828\n",
      "Epoch 390: mean energy = -31.9609\n",
      "Epoch 395: mean energy = -31.8984\n",
      "Epoch 400: mean energy = -31.9219\n",
      "Epoch 405: mean energy = -31.9375\n",
      "Epoch 410: mean energy = -31.9688\n",
      "Epoch 415: mean energy = -31.9531\n",
      "Epoch 420: mean energy = -31.9531\n",
      "Epoch 425: mean energy = -31.8984\n",
      "Epoch 430: mean energy = -31.9375\n",
      "Epoch 435: mean energy = -31.9609\n",
      "Epoch 440: mean energy = -31.9375\n",
      "Epoch 445: mean energy = -31.9609\n",
      "Epoch 450: mean energy = -31.9766\n",
      "Epoch 455: mean energy = -31.9531\n",
      "Epoch 460: mean energy = -31.9688\n",
      "Epoch 465: mean energy = -31.9609\n",
      "Epoch 470: mean energy = -31.9453\n",
      "Epoch 475: mean energy = -31.9141\n",
      "Epoch 480: mean energy = -31.9922\n",
      "Epoch 485: mean energy = -31.9453\n",
      "Epoch 490: mean energy = -31.9375\n",
      "Epoch 495: mean energy = -31.9531\n",
      "Epoch 500: mean energy = -31.9219\n",
      "Epoch 505: mean energy = -31.9609\n",
      "Epoch 510: mean energy = -31.9453\n",
      "Epoch 515: mean energy = -31.9609\n",
      "Epoch 520: mean energy = -31.9062\n",
      "Epoch 525: mean energy = -31.9531\n",
      "Epoch 530: mean energy = -31.9609\n",
      "Epoch 535: mean energy = -31.9766\n",
      "Epoch 540: mean energy = -31.9531\n",
      "Epoch 545: mean energy = -31.9688\n",
      "Epoch 550: mean energy = -31.9453\n",
      "Epoch 555: mean energy = -31.9453\n",
      "Epoch 560: mean energy = -31.9375\n",
      "Epoch 565: mean energy = -31.9297\n",
      "Epoch 570: mean energy = -31.9453\n",
      "Epoch 575: mean energy = -31.9922\n",
      "Epoch 580: mean energy = -31.9453\n",
      "Epoch 585: mean energy = -31.9688\n",
      "Epoch 590: mean energy = -31.9531\n",
      "Epoch 595: mean energy = -31.9453\n",
      "Epoch 600: mean energy = -31.9766\n",
      "Epoch 605: mean energy = -32.0000\n",
      "Epoch 610: mean energy = -31.9688\n",
      "Epoch 615: mean energy = -31.9688\n",
      "Epoch 620: mean energy = -31.9688\n",
      "Epoch 625: mean energy = -31.9609\n",
      "Epoch 630: mean energy = -31.9844\n",
      "Epoch 635: mean energy = -31.9453\n",
      "Epoch 640: mean energy = -31.9609\n",
      "Epoch 645: mean energy = -31.9766\n",
      "Epoch 650: mean energy = -31.9766\n",
      "Epoch 655: mean energy = -31.9688\n",
      "Epoch 660: mean energy = -31.9922\n",
      "Epoch 665: mean energy = -31.9688\n",
      "Epoch 670: mean energy = -31.9688\n",
      "Epoch 675: mean energy = -31.9766\n",
      "Epoch 680: mean energy = -31.9922\n",
      "Epoch 685: mean energy = -31.9609\n",
      "Epoch 690: mean energy = -31.9609\n",
      "Epoch 695: mean energy = -31.9766\n",
      "Epoch 700: mean energy = -31.9609\n",
      "Epoch 705: mean energy = -31.9531\n",
      "Epoch 710: mean energy = -31.9844\n",
      "Epoch 715: mean energy = -31.9844\n",
      "Epoch 720: mean energy = -31.9688\n",
      "Epoch 725: mean energy = -31.9531\n",
      "Epoch 730: mean energy = -31.9844\n",
      "Epoch 735: mean energy = -31.9922\n",
      "Epoch 740: mean energy = -31.9766\n",
      "Epoch 745: mean energy = -31.9844\n",
      "Epoch 750: mean energy = -31.9844\n",
      "Epoch 755: mean energy = -31.9766\n",
      "Epoch 760: mean energy = -31.9922\n",
      "Epoch 765: mean energy = -31.9766\n",
      "Epoch 770: mean energy = -31.9688\n",
      "Epoch 775: mean energy = -31.9844\n",
      "Epoch 780: mean energy = -31.9688\n",
      "Epoch 785: mean energy = -31.9766\n",
      "Epoch 790: mean energy = -31.9844\n",
      "Epoch 795: mean energy = -32.0000\n",
      "Epoch 800: mean energy = -31.9922\n",
      "Epoch 805: mean energy = -31.9688\n",
      "Epoch 810: mean energy = -31.9766\n",
      "Epoch 815: mean energy = -31.9688\n",
      "Epoch 820: mean energy = -31.9766\n",
      "Epoch 825: mean energy = -31.9922\n",
      "Epoch 830: mean energy = -32.0000\n",
      "Epoch 835: mean energy = -31.9844\n",
      "Epoch 840: mean energy = -31.9688\n",
      "Epoch 845: mean energy = -31.9922\n",
      "Epoch 850: mean energy = -31.9609\n",
      "Epoch 855: mean energy = -31.9609\n",
      "Epoch 860: mean energy = -31.9922\n",
      "Epoch 865: mean energy = -31.9766\n",
      "Epoch 870: mean energy = -31.9922\n",
      "Epoch 875: mean energy = -31.9688\n",
      "Epoch 880: mean energy = -31.9844\n",
      "Epoch 885: mean energy = -31.9766\n",
      "Epoch 890: mean energy = -31.9844\n",
      "Epoch 895: mean energy = -31.9922\n",
      "Epoch 900: mean energy = -32.0000\n",
      "Epoch 905: mean energy = -31.9844\n",
      "Epoch 910: mean energy = -31.9922\n",
      "Epoch 915: mean energy = -31.9688\n",
      "Epoch 920: mean energy = -31.9844\n",
      "Epoch 925: mean energy = -31.9609\n",
      "Epoch 930: mean energy = -31.9922\n",
      "Epoch 935: mean energy = -31.9922\n",
      "Epoch 940: mean energy = -31.9766\n",
      "Epoch 945: mean energy = -31.9922\n",
      "Epoch 950: mean energy = -31.9922\n",
      "Epoch 955: mean energy = -32.0000\n",
      "Epoch 960: mean energy = -31.9766\n",
      "Epoch 965: mean energy = -31.9688\n",
      "Epoch 970: mean energy = -31.9766\n",
      "Epoch 975: mean energy = -31.9922\n",
      "Epoch 980: mean energy = -31.9766\n",
      "Epoch 985: mean energy = -31.9922\n",
      "Epoch 990: mean energy = -31.9844\n",
      "Epoch 995: mean energy = -32.0000\n",
      "Epoch 1000: mean energy = -31.9922\n",
      "Epoch 1005: mean energy = -31.9922\n",
      "Epoch 1010: mean energy = -31.9766\n",
      "Epoch 1015: mean energy = -31.9922\n",
      "Epoch 1020: mean energy = -32.0000\n",
      "Epoch 1025: mean energy = -32.0000\n",
      "Epoch 1030: mean energy = -31.9922\n",
      "Epoch 1035: mean energy = -31.9766\n",
      "Epoch 1040: mean energy = -31.9922\n",
      "Epoch 1045: mean energy = -31.9844\n",
      "Epoch 1050: mean energy = -32.0000\n",
      "Epoch 1055: mean energy = -31.9922\n",
      "Epoch 1060: mean energy = -31.9922\n",
      "Epoch 1065: mean energy = -31.9922\n",
      "Epoch 1070: mean energy = -32.0000\n",
      "Epoch 1075: mean energy = -31.9531\n",
      "Epoch 1080: mean energy = -31.9844\n",
      "Epoch 1085: mean energy = -31.9766\n",
      "Epoch 1090: mean energy = -32.0000\n",
      "Epoch 1095: mean energy = -31.9844\n",
      "Epoch 1100: mean energy = -31.9922\n",
      "Epoch 1105: mean energy = -31.9844\n",
      "Epoch 1110: mean energy = -31.9922\n",
      "Epoch 1115: mean energy = -31.9922\n",
      "Epoch 1120: mean energy = -32.0000\n",
      "Epoch 1125: mean energy = -32.0000\n",
      "Epoch 1130: mean energy = -31.9922\n",
      "Epoch 1135: mean energy = -32.0000\n",
      "Epoch 1140: mean energy = -31.9922\n",
      "Epoch 1145: mean energy = -32.0000\n",
      "Epoch 1150: mean energy = -31.9844\n",
      "Epoch 1155: mean energy = -32.0000\n",
      "Epoch 1160: mean energy = -31.9922\n",
      "Epoch 1165: mean energy = -32.0000\n",
      "Epoch 1170: mean energy = -31.9922\n",
      "Epoch 1175: mean energy = -31.9922\n",
      "Epoch 1180: mean energy = -31.9844\n",
      "Epoch 1185: mean energy = -32.0000\n",
      "Epoch 1190: mean energy = -31.9922\n",
      "Epoch 1195: mean energy = -31.9844\n",
      "Epoch 1200: mean energy = -32.0000\n",
      "Epoch 1205: mean energy = -31.9766\n",
      "Epoch 1210: mean energy = -31.9922\n",
      "Epoch 1215: mean energy = -31.9922\n",
      "Epoch 1220: mean energy = -32.0000\n",
      "Epoch 1225: mean energy = -31.9844\n",
      "Epoch 1230: mean energy = -32.0000\n",
      "Epoch 1235: mean energy = -31.9844\n",
      "Epoch 1240: mean energy = -31.9844\n",
      "Epoch 1245: mean energy = -31.9922\n",
      "Epoch 1250: mean energy = -31.9922\n",
      "Epoch 1255: mean energy = -31.9922\n",
      "Epoch 1260: mean energy = -31.9844\n",
      "Epoch 1265: mean energy = -31.9922\n",
      "Epoch 1270: mean energy = -32.0000\n",
      "Epoch 1275: mean energy = -32.0000\n",
      "Epoch 1280: mean energy = -31.9922\n",
      "Epoch 1285: mean energy = -32.0000\n",
      "Epoch 1290: mean energy = -32.0000\n",
      "Epoch 1295: mean energy = -31.9844\n",
      "Epoch 1300: mean energy = -31.9922\n",
      "Epoch 1305: mean energy = -32.0000\n",
      "Epoch 1310: mean energy = -31.9844\n",
      "Epoch 1315: mean energy = -31.9922\n",
      "Epoch 1320: mean energy = -32.0000\n",
      "Epoch 1325: mean energy = -31.9922\n",
      "Epoch 1330: mean energy = -31.9844\n",
      "Epoch 1335: mean energy = -32.0000\n",
      "Epoch 1340: mean energy = -31.9922\n",
      "Epoch 1345: mean energy = -32.0000\n",
      "Epoch 1350: mean energy = -31.9844\n",
      "Epoch 1355: mean energy = -31.9922\n",
      "Epoch 1360: mean energy = -31.9922\n",
      "Epoch 1365: mean energy = -32.0000\n",
      "Epoch 1370: mean energy = -31.9844\n",
      "Epoch 1375: mean energy = -32.0000\n",
      "Epoch 1380: mean energy = -32.0000\n",
      "Epoch 1385: mean energy = -31.9922\n",
      "Epoch 1390: mean energy = -31.9688\n",
      "Epoch 1395: mean energy = -32.0000\n",
      "Epoch 1400: mean energy = -31.9922\n",
      "Epoch 1405: mean energy = -31.9922\n",
      "Epoch 1410: mean energy = -31.9766\n",
      "Epoch 1415: mean energy = -32.0000\n",
      "Epoch 1420: mean energy = -31.9922\n",
      "Epoch 1425: mean energy = -32.0000\n",
      "Epoch 1430: mean energy = -31.9922\n",
      "Epoch 1435: mean energy = -31.9844\n",
      "Epoch 1440: mean energy = -31.9922\n",
      "Epoch 1445: mean energy = -31.9922\n",
      "Epoch 1450: mean energy = -32.0000\n",
      "Epoch 1455: mean energy = -32.0000\n",
      "Epoch 1460: mean energy = -31.9922\n",
      "Epoch 1465: mean energy = -32.0000\n",
      "Epoch 1470: mean energy = -32.0000\n",
      "Epoch 1475: mean energy = -31.9922\n",
      "Epoch 1480: mean energy = -31.9922\n",
      "Epoch 1485: mean energy = -31.9922\n",
      "Epoch 1490: mean energy = -31.9922\n",
      "Epoch 1495: mean energy = -32.0000\n",
      "Epoch 1500: mean energy = -31.9844\n",
      "Epoch 1505: mean energy = -32.0000\n",
      "Epoch 1510: mean energy = -31.9922\n",
      "Epoch 1515: mean energy = -32.0000\n",
      "Epoch 1520: mean energy = -32.0000\n",
      "Epoch 1525: mean energy = -32.0000\n",
      "Epoch 1530: mean energy = -32.0000\n",
      "Epoch 1535: mean energy = -32.0000\n",
      "Epoch 1540: mean energy = -32.0000\n",
      "Epoch 1545: mean energy = -31.9922\n",
      "Epoch 1550: mean energy = -31.9844\n",
      "Epoch 1555: mean energy = -32.0000\n",
      "Epoch 1560: mean energy = -32.0000\n",
      "Epoch 1565: mean energy = -31.9922\n",
      "Epoch 1570: mean energy = -31.9766\n",
      "Epoch 1575: mean energy = -31.9922\n",
      "Epoch 1580: mean energy = -31.9922\n",
      "Epoch 1585: mean energy = -31.9922\n",
      "Epoch 1590: mean energy = -32.0000\n",
      "Epoch 1595: mean energy = -31.9922\n",
      "Epoch 1600: mean energy = -31.9922\n",
      "Epoch 1605: mean energy = -31.9922\n",
      "Epoch 1610: mean energy = -31.9922\n",
      "Epoch 1615: mean energy = -32.0000\n",
      "Epoch 1620: mean energy = -32.0000\n",
      "Epoch 1625: mean energy = -32.0000\n",
      "Epoch 1630: mean energy = -31.9844\n",
      "Epoch 1635: mean energy = -31.9922\n",
      "Epoch 1640: mean energy = -32.0000\n",
      "Epoch 1645: mean energy = -32.0000\n",
      "Epoch 1650: mean energy = -32.0000\n",
      "Epoch 1655: mean energy = -31.9922\n",
      "Epoch 1660: mean energy = -31.9766\n",
      "Epoch 1665: mean energy = -31.9844\n",
      "Epoch 1670: mean energy = -31.9922\n",
      "Epoch 1675: mean energy = -31.9922\n",
      "Epoch 1680: mean energy = -31.9844\n",
      "Epoch 1685: mean energy = -31.9922\n",
      "Epoch 1690: mean energy = -32.0000\n",
      "Epoch 1695: mean energy = -32.0000\n",
      "Epoch 1700: mean energy = -32.0000\n",
      "Epoch 1705: mean energy = -32.0000\n",
      "Epoch 1710: mean energy = -31.9922\n",
      "Epoch 1715: mean energy = -31.9844\n",
      "Epoch 1720: mean energy = -31.9844\n",
      "Epoch 1725: mean energy = -31.9922\n",
      "Epoch 1730: mean energy = -32.0000\n",
      "Epoch 1735: mean energy = -31.9844\n",
      "Epoch 1740: mean energy = -31.9766\n",
      "Epoch 1745: mean energy = -31.9844\n",
      "Epoch 1750: mean energy = -31.9922\n",
      "Epoch 1755: mean energy = -32.0000\n",
      "Epoch 1760: mean energy = -31.9922\n",
      "Epoch 1765: mean energy = -31.9922\n",
      "Epoch 1770: mean energy = -31.9922\n",
      "Epoch 1775: mean energy = -32.0000\n",
      "Epoch 1780: mean energy = -32.0000\n",
      "Epoch 1785: mean energy = -31.9922\n",
      "Epoch 1790: mean energy = -31.9922\n",
      "Epoch 1795: mean energy = -31.9844\n",
      "Epoch 1800: mean energy = -32.0000\n",
      "Epoch 1805: mean energy = -32.0000\n",
      "Epoch 1810: mean energy = -32.0000\n",
      "Epoch 1815: mean energy = -32.0000\n",
      "Epoch 1820: mean energy = -31.9922\n",
      "Epoch 1825: mean energy = -32.0000\n",
      "Epoch 1830: mean energy = -32.0000\n",
      "Epoch 1835: mean energy = -31.9922\n",
      "Epoch 1840: mean energy = -32.0000\n",
      "Epoch 1845: mean energy = -32.0000\n",
      "Epoch 1850: mean energy = -31.9922\n",
      "Epoch 1855: mean energy = -32.0000\n",
      "Epoch 1860: mean energy = -31.9922\n",
      "Epoch 1865: mean energy = -32.0000\n",
      "Epoch 1870: mean energy = -32.0000\n",
      "Epoch 1875: mean energy = -31.9922\n",
      "Epoch 1880: mean energy = -32.0000\n",
      "Epoch 1885: mean energy = -32.0000\n",
      "Epoch 1890: mean energy = -31.9844\n",
      "Epoch 1895: mean energy = -31.9922\n",
      "Epoch 1900: mean energy = -32.0000\n",
      "Epoch 1905: mean energy = -32.0000\n",
      "Epoch 1910: mean energy = -31.9922\n",
      "Epoch 1915: mean energy = -31.9844\n",
      "Epoch 1920: mean energy = -31.9844\n",
      "Epoch 1925: mean energy = -31.9844\n",
      "Epoch 1930: mean energy = -31.9922\n",
      "Epoch 1935: mean energy = -31.9844\n",
      "Epoch 1940: mean energy = -32.0000\n",
      "Epoch 1945: mean energy = -31.9922\n",
      "Epoch 1950: mean energy = -31.9922\n",
      "Epoch 1955: mean energy = -32.0000\n",
      "Epoch 1960: mean energy = -31.9844\n",
      "Epoch 1965: mean energy = -32.0000\n",
      "Epoch 1970: mean energy = -31.9922\n",
      "Epoch 1975: mean energy = -32.0000\n",
      "Epoch 1980: mean energy = -31.9922\n",
      "Epoch 1985: mean energy = -31.9844\n",
      "Epoch 1990: mean energy = -32.0000\n",
      "Epoch 1995: mean energy = -32.0000\n",
      "Epoch 2000: mean energy = -32.0000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    L = 4\n",
    "    ar_net = ARWavefunction(L=L, hidden_dim=128)\n",
    "    train_vmc_logprob(ar_net, L, n_epochs=2000, batch_size=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e13035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# TFIM parameters\n",
    "L = 6            # Number of spins\n",
    "T = 1.0          # Temperature\n",
    "h = 1.0          # Transverse field\n",
    "J = 1.0          # Ising coupling\n",
    "n_samples = 1000 # Monte Carlo samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f21d822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: mean energy = 0.2500\n",
      "Epoch 010: mean energy = -0.6250\n",
      "Epoch 020: mean energy = -1.6250\n",
      "Epoch 030: mean energy = -1.8750\n",
      "Epoch 040: mean energy = -1.2500\n",
      "Epoch 050: mean energy = -0.6250\n",
      "Epoch 060: mean energy = -0.9375\n",
      "Epoch 070: mean energy = -2.6875\n",
      "Epoch 080: mean energy = -4.5000\n",
      "Epoch 090: mean energy = -6.3750\n",
      "Epoch 100: mean energy = -12.2500\n",
      "Epoch 110: mean energy = -14.6875\n",
      "Epoch 120: mean energy = -17.6250\n",
      "Epoch 130: mean energy = -20.3125\n",
      "Epoch 140: mean energy = -22.0000\n",
      "Epoch 150: mean energy = -23.3125\n",
      "Epoch 160: mean energy = -23.2500\n",
      "Epoch 170: mean energy = -24.7500\n",
      "Epoch 180: mean energy = -25.0000\n",
      "Epoch 190: mean energy = -26.6250\n",
      "Epoch 200: mean energy = -28.3750\n",
      "Epoch 210: mean energy = -28.6250\n",
      "Epoch 220: mean energy = -28.1250\n",
      "Epoch 230: mean energy = -27.6250\n",
      "Epoch 240: mean energy = -27.6250\n",
      "Epoch 250: mean energy = -27.2500\n",
      "Epoch 260: mean energy = -26.6250\n",
      "Epoch 270: mean energy = -26.2500\n",
      "Epoch 280: mean energy = -25.7500\n",
      "Epoch 290: mean energy = -25.1250\n",
      "Epoch 300: mean energy = -26.1250\n",
      "Epoch 310: mean energy = -25.5000\n",
      "Epoch 320: mean energy = -25.2500\n",
      "Epoch 330: mean energy = -25.3750\n",
      "Epoch 340: mean energy = -24.8750\n",
      "Epoch 350: mean energy = -24.3750\n",
      "Epoch 360: mean energy = -25.3750\n",
      "Epoch 370: mean energy = -24.8750\n",
      "Epoch 380: mean energy = -25.0000\n",
      "Epoch 390: mean energy = -24.6250\n",
      "Epoch 400: mean energy = -24.6250\n",
      "Epoch 410: mean energy = -24.3750\n",
      "Epoch 420: mean energy = -25.2500\n",
      "Epoch 430: mean energy = -24.3750\n",
      "Epoch 440: mean energy = -25.0000\n",
      "Epoch 450: mean energy = -24.3750\n",
      "Epoch 460: mean energy = -24.7500\n",
      "Epoch 470: mean energy = -24.7500\n",
      "Epoch 480: mean energy = -24.5000\n",
      "Epoch 490: mean energy = -24.3750\n",
      "Epoch 500: mean energy = -24.1250\n",
      "Epoch 510: mean energy = -24.3750\n",
      "Epoch 520: mean energy = -24.2500\n",
      "Epoch 530: mean energy = -24.2500\n",
      "Epoch 540: mean energy = -24.8750\n",
      "Epoch 550: mean energy = -24.3750\n",
      "Epoch 560: mean energy = -24.3750\n",
      "Epoch 570: mean energy = -24.1250\n",
      "Epoch 580: mean energy = -24.1250\n",
      "Epoch 590: mean energy = -24.0000\n",
      "Epoch 600: mean energy = -24.0000\n",
      "Epoch 610: mean energy = -24.1250\n",
      "Epoch 620: mean energy = -24.1250\n",
      "Epoch 630: mean energy = -24.1250\n",
      "Epoch 640: mean energy = -24.0000\n",
      "Epoch 650: mean energy = -24.1250\n",
      "Epoch 660: mean energy = -24.0000\n",
      "Epoch 670: mean energy = -24.2500\n",
      "Epoch 680: mean energy = -24.1250\n",
      "Epoch 690: mean energy = -24.1250\n",
      "Epoch 700: mean energy = -24.2500\n",
      "Epoch 710: mean energy = -24.1250\n",
      "Epoch 720: mean energy = -24.0000\n",
      "Epoch 730: mean energy = -24.0000\n",
      "Epoch 740: mean energy = -24.0000\n",
      "Epoch 750: mean energy = -24.1250\n",
      "Epoch 760: mean energy = -24.1250\n",
      "Epoch 770: mean energy = -24.0000\n",
      "Epoch 780: mean energy = -24.0000\n",
      "Epoch 790: mean energy = -24.0000\n",
      "Epoch 800: mean energy = -24.0000\n",
      "Epoch 810: mean energy = -24.0000\n",
      "Epoch 820: mean energy = -24.0000\n",
      "Epoch 830: mean energy = -24.0000\n",
      "Epoch 840: mean energy = -24.0000\n",
      "Epoch 850: mean energy = -24.0000\n",
      "Epoch 860: mean energy = -24.0000\n",
      "Epoch 870: mean energy = -24.0000\n",
      "Epoch 880: mean energy = -24.0000\n",
      "Epoch 890: mean energy = -24.1250\n",
      "Epoch 900: mean energy = -24.0000\n",
      "Epoch 910: mean energy = -24.0000\n",
      "Epoch 920: mean energy = -24.0000\n",
      "Epoch 930: mean energy = -24.0000\n",
      "Epoch 940: mean energy = -24.0000\n",
      "Epoch 950: mean energy = -24.1250\n",
      "Epoch 960: mean energy = -24.0000\n",
      "Epoch 970: mean energy = -24.0000\n",
      "Epoch 980: mean energy = -24.0000\n",
      "Epoch 990: mean energy = -24.0000\n",
      "Epoch 1000: mean energy = -24.0000\n",
      "Epoch 1010: mean energy = -24.1250\n",
      "Epoch 1020: mean energy = -24.1250\n",
      "Epoch 1030: mean energy = -24.0000\n",
      "Epoch 1040: mean energy = -24.0000\n",
      "Epoch 1050: mean energy = -24.0000\n",
      "Epoch 1060: mean energy = -24.0000\n",
      "Epoch 1070: mean energy = -24.1250\n",
      "Epoch 1080: mean energy = -24.0000\n",
      "Epoch 1090: mean energy = -24.1250\n",
      "Epoch 1100: mean energy = -24.0000\n",
      "Epoch 1110: mean energy = -24.0000\n",
      "Epoch 1120: mean energy = -24.2500\n",
      "Epoch 1130: mean energy = -24.0000\n",
      "Epoch 1140: mean energy = -24.1250\n",
      "Epoch 1150: mean energy = -24.2500\n",
      "Epoch 1160: mean energy = -24.0000\n",
      "Epoch 1170: mean energy = -24.0000\n",
      "Epoch 1180: mean energy = -24.0000\n",
      "Epoch 1190: mean energy = -24.1250\n",
      "Epoch 1200: mean energy = -24.0000\n",
      "Epoch 1210: mean energy = -24.0000\n",
      "Epoch 1220: mean energy = -24.0000\n",
      "Epoch 1230: mean energy = -24.1250\n",
      "Epoch 1240: mean energy = -24.0000\n",
      "Epoch 1250: mean energy = -24.0000\n",
      "Epoch 1260: mean energy = -24.1250\n",
      "Epoch 1270: mean energy = -24.0000\n",
      "Epoch 1280: mean energy = -24.0000\n",
      "Epoch 1290: mean energy = -24.0000\n",
      "Epoch 1300: mean energy = -24.1250\n",
      "Epoch 1310: mean energy = -24.0000\n",
      "Epoch 1320: mean energy = -24.0000\n",
      "Epoch 1330: mean energy = -24.0000\n",
      "Epoch 1340: mean energy = -24.0000\n",
      "Epoch 1350: mean energy = -24.1250\n",
      "Epoch 1360: mean energy = -24.0000\n",
      "Epoch 1370: mean energy = -24.0000\n",
      "Epoch 1380: mean energy = -24.0000\n",
      "Epoch 1390: mean energy = -24.0000\n",
      "Epoch 1400: mean energy = -24.0000\n",
      "Epoch 1410: mean energy = -24.0000\n",
      "Epoch 1420: mean energy = -24.0000\n",
      "Epoch 1430: mean energy = -24.1250\n",
      "Epoch 1440: mean energy = -24.0000\n",
      "Epoch 1450: mean energy = -24.1250\n",
      "Epoch 1460: mean energy = -24.0000\n",
      "Epoch 1470: mean energy = -24.1250\n",
      "Epoch 1480: mean energy = -24.0000\n",
      "Epoch 1490: mean energy = -24.0000\n",
      "Epoch 1500: mean energy = -24.0000\n",
      "Epoch 1510: mean energy = -24.0000\n",
      "Epoch 1520: mean energy = -24.0000\n",
      "Epoch 1530: mean energy = -24.0000\n",
      "Epoch 1540: mean energy = -24.0000\n",
      "Epoch 1550: mean energy = -24.1250\n",
      "Epoch 1560: mean energy = -24.0000\n",
      "Epoch 1570: mean energy = -24.0000\n",
      "Epoch 1580: mean energy = -24.0000\n",
      "Epoch 1590: mean energy = -24.0000\n",
      "Epoch 1600: mean energy = -24.0000\n",
      "Epoch 1610: mean energy = -24.0000\n",
      "Epoch 1620: mean energy = -24.0000\n",
      "Epoch 1630: mean energy = -24.0000\n",
      "Epoch 1640: mean energy = -24.0000\n",
      "Epoch 1650: mean energy = -24.0000\n",
      "Epoch 1660: mean energy = -24.0000\n",
      "Epoch 1670: mean energy = -24.1250\n",
      "Epoch 1680: mean energy = -24.0000\n",
      "Epoch 1690: mean energy = -24.0000\n",
      "Epoch 1700: mean energy = -24.0000\n",
      "Epoch 1710: mean energy = -24.0000\n",
      "Epoch 1720: mean energy = -24.0000\n",
      "Epoch 1730: mean energy = -24.0000\n",
      "Epoch 1740: mean energy = -24.0000\n",
      "Epoch 1750: mean energy = -24.0000\n",
      "Epoch 1760: mean energy = -24.0000\n",
      "Epoch 1770: mean energy = -24.0000\n",
      "Epoch 1780: mean energy = -24.0000\n",
      "Epoch 1790: mean energy = -24.0000\n",
      "Epoch 1800: mean energy = -24.0000\n",
      "Epoch 1810: mean energy = -24.0000\n",
      "Epoch 1820: mean energy = -24.0000\n",
      "Epoch 1830: mean energy = -24.0000\n",
      "Epoch 1840: mean energy = -24.0000\n",
      "Epoch 1850: mean energy = -24.0000\n",
      "Epoch 1860: mean energy = -24.0000\n",
      "Epoch 1870: mean energy = -24.0000\n",
      "Epoch 1880: mean energy = -24.0000\n",
      "Epoch 1890: mean energy = -24.0000\n",
      "Epoch 1900: mean energy = -24.0000\n",
      "Epoch 1910: mean energy = -24.0000\n",
      "Epoch 1920: mean energy = -24.0000\n",
      "Epoch 1930: mean energy = -24.0000\n",
      "Epoch 1940: mean energy = -24.0000\n",
      "Epoch 1950: mean energy = -24.0000\n",
      "Epoch 1960: mean energy = -24.0000\n",
      "Epoch 1970: mean energy = -24.0000\n",
      "Epoch 1980: mean energy = -24.0000\n",
      "Epoch 1990: mean energy = -24.0000\n",
      "Epoch 2000: mean energy = -24.0000\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Main\n",
    "# -------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    L = 4\n",
    "    T = 0.1\n",
    "    hidden_dim = 128\n",
    "    model = ARWavefunctionTemp(L=L, hidden_dim=hidden_dim)\n",
    "    configs = train_vmc_finiteT(model, L, T, n_epochs=2000, batch_size=64, lr=1e-3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079191bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# TFIM parameters\n",
    "L = 6            # Number of spins\n",
    "T = 1.0          # Temperature\n",
    "h = 1.0          # Transverse field\n",
    "J = 1.0          # Ising coupling\n",
    "n_samples = 1000 # Monte Carlo samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81bd274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARWavefunction(nn.Module):\n",
    "    def __init__(self, L, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Simple autoregressive layers\n",
    "        self.fc1 = nn.Linear(L, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, L)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, L) with spins {-1, 1}\n",
    "        x_in = (x + 1) / 2  # Map to {0,1} for network\n",
    "        h = F.relu(self.fc1(x_in))\n",
    "        logits = self.fc2(h)\n",
    "        # Probabilities for spin up\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29873759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ar(model, L, n_samples):\n",
    "    samples = torch.zeros((n_samples, L), device=device)\n",
    "    \n",
    "    for i in range(L):\n",
    "        probs = model(samples)[:, i]\n",
    "        spins = torch.bernoulli(probs) * 2 - 1  # {0,1} -> {-1,1}\n",
    "        samples[:, i] = spins\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_tfim(configs, J, h):\n",
    "    # configs: (n_samples, L)\n",
    "    n_samples, L = configs.shape\n",
    "    E = torch.zeros(n_samples, device=configs.device)\n",
    "    \n",
    "    # ZZ interaction (periodic boundary)\n",
    "    E -= J * torch.sum(configs * torch.roll(configs, shifts=-1, dims=1), dim=1)\n",
    "    \n",
    "    # Transverse field <X>\n",
    "    # <X> ~ sum over spin flips\n",
    "    for i in range(L):\n",
    "        flipped = configs.clone()\n",
    "        flipped[:, i] *= -1\n",
    "        # Energy contribution from flip\n",
    "        E -= h * torch.ones(n_samples, device=configs.device)\n",
    "    \n",
    "    return E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c57f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = -3.9393, Energy = -5.9440\n",
      "Epoch 20, Loss = -25.9357, Energy = -11.2120\n",
      "Epoch 40, Loss = -69.4114, Energy = -12.0000\n",
      "Epoch 60, Loss = -122.4473, Energy = -12.0000\n",
      "Epoch 80, Loss = -122.5236, Energy = -12.0000\n",
      "Epoch 100, Loss = -122.5239, Energy = -12.0000\n",
      "Epoch 120, Loss = -122.5239, Energy = -12.0000\n",
      "Epoch 140, Loss = -122.5239, Energy = -12.0000\n",
      "Epoch 160, Loss = -122.5239, Energy = -12.0000\n",
      "Epoch 180, Loss = -122.5239, Energy = -12.0000\n"
     ]
    }
   ],
   "source": [
    "model = ARWavefunction(L).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    samples = sample_ar(model, L, n_samples)\n",
    "    E = energy_tfim(samples, J, h)\n",
    "    \n",
    "    # Free energy loss: <E> - T * <log_prob>\n",
    "    x_in = (samples + 1) / 2\n",
    "    probs = model(samples)\n",
    "    log_prob = torch.sum(samples * torch.log(probs + 1e-8) + (1-samples)/2 * torch.log(1-probs + 1e-8), dim=1)\n",
    "    \n",
    "    loss = torch.mean(E - T * log_prob)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss = {loss.item():.4f}, Energy = {E.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f00048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
